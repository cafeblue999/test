{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cafeblue999/test/blob/master/simple2_train_tpu_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAQL60g2jIcK",
        "outputId": "333326f7-9900-4efd-be0a-542596ab60a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "2025-04-13 15:04:07 INFO: ==== Loaded Configuration ====\n",
            "2025-04-13 15:04:07 INFO: Config file: /content/drive/My Drive/sgf/config_py.ini\n",
            "2025-04-13 15:04:07 INFO: BOARD_SIZE: 19\n",
            "2025-04-13 15:04:07 INFO: HISTORY_LENGTH: 8\n",
            "2025-04-13 15:04:07 INFO: NUM_CHANNELS: 17\n",
            "2025-04-13 15:04:07 INFO: NUM_ACTIONS: 362\n",
            "2025-04-13 15:04:07 INFO: num_residual_blocks: 20\n",
            "2025-04-13 15:04:07 INFO: model_channels: 256\n",
            "2025-04-13 15:04:07 INFO: num_epochs: 1000\n",
            "2025-04-13 15:04:07 INFO: batch_size: 256\n",
            "2025-04-13 15:04:07 INFO: learning_rate: 0.001\n",
            "2025-04-13 15:04:07 INFO: patience: 1\n",
            "2025-04-13 15:04:07 INFO: factor: 0.8\n",
            "2025-04-13 15:04:07 INFO: number_max_files: 256\n",
            "2025-04-13 15:04:07 INFO: ===============================\n",
            "2025-04-13 15:04:07 INFO: === Starting Training and Validation Loop ===\n",
            "2025-04-13 15:04:07 INFO: Running on TPU device: xla:0\n",
            "2025-04-13 15:04:07 INFO: Test dataset pickle /content/drive/My Drive/sgf/test/test_dataset.pkl already exists. Loading it directly...\n",
            "2025-04-13 15:04:23 INFO: Loaded dataset from /content/drive/My Drive/sgf/test/test_dataset.pkl\n",
            "2025-04-13 15:04:23 INFO: No checkpoint found. Starting from scratch.\n",
            "2025-04-13 15:04:23 INFO: Initial best_policy_accuracy: 0.00000\n",
            "2025-04-13 15:04:23 INFO: Current learning rate : 0.00100000\n",
            "2025-04-13 15:04:23 INFO: Regenerated the random order of all SGF files.\n",
            "2025-04-13 15:04:23 INFO: Selected 256 SGF files.\n",
            "2025-04-13 15:06:11 INFO: Training dataset cycle created. Total samples: 645368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|▍         |    100/  2521 [01:49<35:15,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch    1～ 100 policy accuracy average: 0.0083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|▊         |    200/  2521 [03:19<34:51,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  101～ 200 policy accuracy average: 0.0112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  12%|█▏        |    300/  2521 [04:49<33:22,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  201～ 300 policy accuracy average: 0.0194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  16%|█▌        |    400/  2521 [06:19<31:53,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  301～ 400 policy accuracy average: 0.0316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|█▉        |    500/  2521 [07:50<30:32,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  401～ 500 policy accuracy average: 0.0446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|██▍       |    600/  2521 [09:20<29:03,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  501～ 600 policy accuracy average: 0.0443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|██▊       |    700/  2521 [10:51<27:25,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  601～ 700 policy accuracy average: 0.0468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  32%|███▏      |    800/  2521 [12:21<25:25,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  701～ 800 policy accuracy average: 0.0507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  36%|███▌      |    900/  2521 [13:50<23:53,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  801～ 900 policy accuracy average: 0.0619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  40%|███▉      |   1000/  2521 [15:20<22:10,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  901～1000 policy accuracy average: 0.0583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  44%|████▎     |   1100/  2521 [16:47<20:46,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1001～1100 policy accuracy average: 0.0663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  48%|████▊     |   1200/  2521 [18:16<19:54,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1101～1200 policy accuracy average: 0.0672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  50%|█████     |   1268/  2521 [19:18<18:53,  1.11it/s]"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "train.py\n",
        "(説明略)\n",
        "\"\"\"\n",
        "\n",
        "# ===== 固定定義：環境切り替え用フラグ =====\n",
        "USE_TPU = True\n",
        "USE_COLAB = True\n",
        "\n",
        "# ------------------------------\n",
        "# 必要なライブラリのインポート\n",
        "# ------------------------------\n",
        "import os, re, pickle, zipfile, random, numpy as np, configparser, argparse, functools\n",
        "from tqdm import tqdm\n",
        "import torch, torch.nn as nn, torch.optim as optim, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.distributed as dist\n",
        "\n",
        "if USE_TPU:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    import torch_xla.distributed.xla_backend\n",
        "\n",
        "if USE_COLAB:\n",
        "    try:\n",
        "        os.system(\"fusermount -u /content/drive\")\n",
        "    except Exception as e:\n",
        "        print(\"Google Drive unmount failed:\", e)\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "    except ImportError:\n",
        "        print(\"Google Colab module not found.\")\n",
        "\n",
        "bar_fmt = \"{l_bar}{bar}| {n:>6d}/{total:>6d} [{elapsed}<{remaining}, {rate_fmt}]\"\n",
        "\n",
        "# ==============================\n",
        "# デバイス設定\n",
        "# ==============================\n",
        "if USE_TPU:\n",
        "    device = xm.xla_device()\n",
        "    if not dist.is_initialized():\n",
        "        dist.init_process_group(\"xla\", init_method='xla://')\n",
        "else:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==============================\n",
        "# ディレクトリ設定\n",
        "# ==============================\n",
        "if USE_COLAB:\n",
        "    BASE_DIR = \"/content/drive/My Drive/sgf\"\n",
        "    TRAIN_SGF_DIR = os.path.join(BASE_DIR, \"train_sgf_KK\")\n",
        "    VAL_SGF_DIR = os.path.join(BASE_DIR, \"test\")\n",
        "    TEST_SGFS_ZIP = os.path.join(VAL_SGF_DIR, \"test_sgfs.zip\")\n",
        "    MODEL_OUTPUT_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "    CHECKPOINT_FILE = os.path.join(BASE_DIR, \"checkpoint2.pt\")\n",
        "else:\n",
        "    BASE_DIR = r\"D:\\igo\\simple2\"\n",
        "    TRAIN_SGF_DIR = os.path.join(BASE_DIR, \"train_sgf\")\n",
        "    VAL_SGF_DIR = os.path.join(BASE_DIR, \"test\")\n",
        "    TEST_SGFS_ZIP = os.path.join(VAL_SGF_DIR, \"test_sgfs.zip\")\n",
        "    MODEL_OUTPUT_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "    CHECKPOINT_FILE = os.path.join(BASE_DIR, \"checkpoint2.pt\")\n",
        "\n",
        "if not os.path.exists(MODEL_OUTPUT_DIR):\n",
        "    os.makedirs(MODEL_OUTPUT_DIR)\n",
        "\n",
        "# ==============================\n",
        "# DummyLogger\n",
        "# ==============================\n",
        "from datetime import datetime, timedelta, timezone\n",
        "JST = timezone(timedelta(hours=9), 'JST')\n",
        "class DummyLogger:\n",
        "    def info(self, message, *args, **kwargs):\n",
        "        timestamp = datetime.now(JST).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"{timestamp} INFO: {message}\", *args, **kwargs)\n",
        "    def warning(self, message, *args, **kwargs):\n",
        "        timestamp = datetime.now(JST).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"{timestamp} WARNING: {message}\", *args, **kwargs)\n",
        "    def error(self, message, *args, **kwargs):\n",
        "        timestamp = datetime.now(JST).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"{timestamp} ERROR: {message}\", *args, **kwargs)\n",
        "\n",
        "sgf_logger = DummyLogger()\n",
        "train_logger = DummyLogger()\n",
        "\n",
        "# ==============================\n",
        "# 設定ファイル読み込み\n",
        "# ==============================\n",
        "def load_config(config_path):\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read(config_path)\n",
        "    try:\n",
        "        BOARD_SIZE = int(config.get(\"BOARD\", \"board_size\", fallback=\"19\"))\n",
        "        HISTORY_LENGTH = int(config.get(\"DATA\", \"history_length\", fallback=\"8\"))\n",
        "        NUM_CHANNELS = 2 * HISTORY_LENGTH + 1\n",
        "        NUM_ACTIONS = BOARD_SIZE * BOARD_SIZE + 1\n",
        "        num_residual_blocks = int(config.get(\"MODEL\", \"num_residual_blocks\", fallback=\"20\"))\n",
        "        model_channels = int(config.get(\"MODEL\", \"model_channels\", fallback=\"256\"))\n",
        "        num_epochs = int(config.get(\"TRAIN\", \"num_epochs\", fallback=\"100\"))\n",
        "        batch_size = int(config.get(\"TRAIN\", \"batch_size\", fallback=\"256\"))\n",
        "        learning_rate = float(config.get(\"TRAIN\", \"learning_rate\", fallback=\"0.001\"))\n",
        "        patience = int(config.get(\"TRAIN\", \"patience\", fallback=\"10\"))\n",
        "        factor = float(config.get(\"TRAIN\", \"factor\"))\n",
        "        number_max_files = int(config.get(\"TRAIN\", \"number_max_files\", fallback=\"256\"))\n",
        "    except Exception as e:\n",
        "        train_logger.error(f\"Error reading configuration: {e}\")\n",
        "        exit(1)\n",
        "    return {\n",
        "        \"BOARD_SIZE\": BOARD_SIZE,\n",
        "        \"HISTORY_LENGTH\": HISTORY_LENGTH,\n",
        "        \"NUM_CHANNELS\": NUM_CHANNELS,\n",
        "        \"NUM_ACTIONS\": NUM_ACTIONS,\n",
        "        \"num_residual_blocks\": num_residual_blocks,\n",
        "        \"model_channels\": model_channels,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"patience\": patience,\n",
        "        \"factor\": factor,\n",
        "        \"number_max_files\": number_max_files\n",
        "    }\n",
        "\n",
        "CONFIG_PATH = os.path.join(BASE_DIR, \"config_py.ini\")\n",
        "config_params = load_config(CONFIG_PATH)\n",
        "\n",
        "BOARD_SIZE = config_params[\"BOARD_SIZE\"]\n",
        "HISTORY_LENGTH = config_params[\"HISTORY_LENGTH\"]\n",
        "NUM_CHANNELS = config_params[\"NUM_CHANNELS\"]\n",
        "NUM_ACTIONS = config_params[\"NUM_ACTIONS\"]\n",
        "num_residual_blocks = config_params[\"num_residual_blocks\"]\n",
        "model_channels = config_params[\"model_channels\"]\n",
        "num_epochs = config_params[\"num_epochs\"]\n",
        "batch_size = config_params[\"batch_size\"]\n",
        "learning_rate = config_params[\"learning_rate\"]\n",
        "patience = config_params[\"patience\"]\n",
        "factor = config_params[\"factor\"]\n",
        "number_max_files = config_params[\"number_max_files\"]\n",
        "\n",
        "train_logger.info(\"==== Loaded Configuration ====\")\n",
        "train_logger.info(f\"Config file: {CONFIG_PATH}\")\n",
        "train_logger.info(f\"BOARD_SIZE: {BOARD_SIZE}\")\n",
        "train_logger.info(f\"HISTORY_LENGTH: {HISTORY_LENGTH}\")\n",
        "train_logger.info(f\"NUM_CHANNELS: {NUM_CHANNELS}\")\n",
        "train_logger.info(f\"NUM_ACTIONS: {NUM_ACTIONS}\")\n",
        "train_logger.info(f\"num_residual_blocks: {num_residual_blocks}\")\n",
        "train_logger.info(f\"model_channels: {model_channels}\")\n",
        "train_logger.info(f\"num_epochs: {num_epochs}\")\n",
        "train_logger.info(f\"batch_size: {batch_size}\")\n",
        "train_logger.info(f\"learning_rate: {learning_rate}\")\n",
        "train_logger.info(f\"patience: {patience}\")\n",
        "train_logger.info(f\"factor: {factor}\")\n",
        "train_logger.info(f\"number_max_files: {number_max_files}\")\n",
        "train_logger.info(\"===============================\")\n",
        "\n",
        "# ==============================\n",
        "# ネットワーク定義\n",
        "# ==============================\n",
        "# (ここでは、ResidualBlock, DilatedResidualBlock, SelfAttention, EnhancedResNetPolicyValueNetwork の定義を省略せずそのまま置く)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += residual\n",
        "        return F.relu(out)\n",
        "\n",
        "class DilatedResidualBlock(nn.Module):\n",
        "    def __init__(self, channels, dilation=2):\n",
        "        super(DilatedResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=dilation, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += residual\n",
        "        return F.relu(out)\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "    def forward(self, x):\n",
        "        batch, C, H, W = x.size()\n",
        "        proj_query = self.query_conv(x).view(batch, -1, H * W).permute(0, 2, 1)\n",
        "        proj_key = self.key_conv(x).view(batch, -1, H * W)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = self.softmax(energy)\n",
        "        proj_value = self.value_conv(x).view(batch, -1, H * W)\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch, C, H, W)\n",
        "        return self.gamma * out + x\n",
        "\n",
        "class EnhancedResNetPolicyValueNetwork(nn.Module):\n",
        "    def __init__(self, board_size, num_channels, num_residual_blocks, in_channels):\n",
        "        super(EnhancedResNetPolicyValueNetwork, self).__init__()\n",
        "        self.board_size = board_size\n",
        "        self.conv_input = nn.Conv2d(in_channels, num_channels, kernel_size=3, padding=1)\n",
        "        self.bn_input = nn.BatchNorm2d(num_channels)\n",
        "        blocks = []\n",
        "        for i in range(num_residual_blocks):\n",
        "            if i % 4 == 0:\n",
        "                blocks.append(DilatedResidualBlock(num_channels, dilation=2))\n",
        "            else:\n",
        "                blocks.append(ResidualBlock(num_channels))\n",
        "        self.residual_blocks = nn.Sequential(*blocks)\n",
        "        self.attention = SelfAttention(num_channels)\n",
        "        self.conv_policy = nn.Conv2d(num_channels, 2, kernel_size=1)\n",
        "        self.bn_policy = nn.BatchNorm2d(2)\n",
        "        self.dropout_policy = nn.Dropout(p=0.5)\n",
        "        self.fc_policy = nn.Linear(2 * board_size * board_size, NUM_ACTIONS)\n",
        "        self.conv_value = nn.Conv2d(num_channels, 1, kernel_size=1)\n",
        "        self.bn_value = nn.BatchNorm2d(1)\n",
        "        self.fc_value1 = nn.Linear(board_size * board_size, 64)\n",
        "        self.dropout_value = nn.Dropout(p=0.5)\n",
        "        self.fc_value2 = nn.Linear(64, 2)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn_input(self.conv_input(x)))\n",
        "        x = self.residual_blocks(x)\n",
        "        x = self.attention(x)\n",
        "        # Policy head\n",
        "        p = F.relu(self.bn_policy(self.conv_policy(x)))\n",
        "        p = self.dropout_policy(p)\n",
        "        p = p.view(p.size(0), -1)\n",
        "        p = self.fc_policy(p)\n",
        "        p = F.log_softmax(p, dim=1)\n",
        "        # Value head\n",
        "        v = F.relu(self.bn_value(self.conv_value(x)))\n",
        "        v = v.view(v.size(0), -1)\n",
        "        v = F.relu(self.fc_value1(v))\n",
        "        v = self.dropout_value(v)\n",
        "        out = self.fc_value2(v)\n",
        "        value = torch.tanh(out[:, 0])\n",
        "        margin = out[:, 1]\n",
        "        return p, (value, margin)\n",
        "\n",
        "# ==============================\n",
        "# SGFパーサー＆前処理関数\n",
        "# ==============================\n",
        "def parse_sgf(sgf_text):\n",
        "    sgf_text = sgf_text.strip()\n",
        "    if sgf_text.startswith('(') and sgf_text.endswith(')'):\n",
        "        sgf_text = sgf_text[1:-1]\n",
        "    parts = [part for part in sgf_text.split(';') if part.strip()]\n",
        "    nodes = []\n",
        "    prop_pattern = re.compile(r'([A-Z]+)\\[([^\\]]*)\\]')\n",
        "    for part in parts:\n",
        "        props = {}\n",
        "        for m in prop_pattern.finditer(part):\n",
        "            key = m.group(1).encode('utf-8')\n",
        "            value = m.group(2)\n",
        "            props[key] = [value.encode('utf-8')]\n",
        "        nodes.append(props)\n",
        "    if not nodes:\n",
        "        raise ValueError(\"No nodes found in SGF file\")\n",
        "    return {\"root\": nodes[0], \"nodes\": nodes[1:]}\n",
        "\n",
        "def build_input_from_history(history, current_player, board_size, history_length):\n",
        "    channels = []\n",
        "    for i in range(history_length):\n",
        "        if i < len(history):\n",
        "            board = history[-(i+1)]\n",
        "        else:\n",
        "            board = np.zeros((board_size, board_size), dtype=np.float32)\n",
        "        channels.append((board == 1).astype(np.float32))\n",
        "        channels.append((board == 2).astype(np.float32))\n",
        "    current_plane = np.ones((board_size, board_size), dtype=np.float32) if current_player == 1 else np.zeros((board_size, board_size), dtype=np.float32)\n",
        "    channels.append(current_plane)\n",
        "    return np.stack(channels, axis=0)\n",
        "\n",
        "def apply_dihedral_transform(input_array, transform_id):\n",
        "    if transform_id < 4:\n",
        "        return np.rot90(input_array, k=transform_id, axes=(1,2))\n",
        "    else:\n",
        "        flipped = np.flip(input_array, axis=2)\n",
        "        return np.rot90(flipped, k=transform_id-4, axes=(1,2))\n",
        "\n",
        "def transform_policy(target_policy, transform_id, board_size):\n",
        "    idx = np.argmax(target_policy)\n",
        "    if idx == board_size * board_size:\n",
        "        return target_policy\n",
        "    row = idx // board_size\n",
        "    col = idx % board_size\n",
        "    board = np.zeros((board_size, board_size), dtype=np.float32)\n",
        "    board[row, col] = 1.0\n",
        "    transformed_board = apply_dihedral_transform(board[np.newaxis, ...], transform_id)[0]\n",
        "    new_idx = np.argmax(transformed_board)\n",
        "    new_policy = np.zeros_like(target_policy)\n",
        "    new_policy[new_idx] = 1.0\n",
        "    return new_policy\n",
        "\n",
        "# ==============================\n",
        "# 盤面クラス\n",
        "# ==============================\n",
        "class Board:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.board = np.zeros((size, size), dtype=np.int8)\n",
        "    def neighbors(self, row, col):\n",
        "        for dr, dc in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
        "            r, c = row+dr, col+dc\n",
        "            if 0 <= r < self.size and 0 <= c < self.size:\n",
        "                yield (r, c)\n",
        "    def get_group(self, row, col):\n",
        "        color = self.board[row, col]\n",
        "        group = []\n",
        "        liberties = set()\n",
        "        stack = [(row, col)]\n",
        "        visited = set()\n",
        "        while stack:\n",
        "            r, c = stack.pop()\n",
        "            if (r, c) in visited:\n",
        "                continue\n",
        "            visited.add((r, c))\n",
        "            group.append((r, c))\n",
        "            for nr, nc in self.neighbors(r, c):\n",
        "                if self.board[nr, nc] == 0:\n",
        "                    liberties.add((nr, nc))\n",
        "                elif self.board[nr, nc] == color and (nr, nc) not in visited:\n",
        "                    stack.append((nr, nc))\n",
        "        return group, liberties\n",
        "    def play(self, move, color):\n",
        "        row, col = move\n",
        "        if self.board[row, col] != 0:\n",
        "            raise ValueError(\"Illegal move: position already occupied\")\n",
        "        stone = 1 if color=='b' else 2\n",
        "        self.board[row, col] = stone\n",
        "        opponent = 2 if stone==1 else 1\n",
        "        for nr, nc in self.neighbors(row, col):\n",
        "            if self.board[nr, nc] == opponent:\n",
        "                group, liberties = self.get_group(nr, nc)\n",
        "                if len(liberties) == 0:\n",
        "                    for r, c in group:\n",
        "                        self.board[r, c] = 0\n",
        "        group, liberties = self.get_group(row, col)\n",
        "        if len(liberties) == 0:\n",
        "            for r, c in group:\n",
        "                self.board[r, c] = 0\n",
        "\n",
        "# ==============================\n",
        "# Datasetクラス\n",
        "# ==============================\n",
        "class AlphaZeroSGFDatasetPreloaded(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        inp, pol, val, mar = self.samples[idx]\n",
        "        board_tensor = torch.tensor(inp, dtype=torch.float32).view(NUM_CHANNELS, BOARD_SIZE, BOARD_SIZE)\n",
        "        target_policy_tensor = torch.tensor(pol, dtype=torch.float32)\n",
        "        target_value_tensor = torch.tensor(val, dtype=torch.float32)\n",
        "        target_margin_tensor = torch.tensor(mar, dtype=torch.float32)\n",
        "        return board_tensor, target_policy_tensor, target_value_tensor, target_margin_tensor\n",
        "\n",
        "# ==============================\n",
        "# SGFからサンプル生成関数\n",
        "# ==============================\n",
        "def process_sgf_to_samples_from_text(sgf_src, board_size, history_length, augment_all):\n",
        "    samples = []\n",
        "    try:\n",
        "        sgf_data = parse_sgf(sgf_src)\n",
        "    except Exception as e:\n",
        "        sgf_logger.error(f\"Error processing SGF text: {e}\")\n",
        "        return samples\n",
        "    root = sgf_data[\"root\"]\n",
        "    try:\n",
        "        sz = int(root.get(b'SZ')[0].decode('utf-8'))\n",
        "    except Exception:\n",
        "        sz = board_size\n",
        "    result_prop = root.get(b'RE') if b'RE' in root else None\n",
        "    result_str = result_prop[0].decode('utf-8') if result_prop and len(result_prop)>0 else \"不明\"\n",
        "    target_value = 1.0 if result_str.startswith(\"B+\") else -1.0 if result_str.startswith(\"W+\") else 0.0\n",
        "    try:\n",
        "        target_margin = float(result_str[2:]) if result_str[2:] else 0.0\n",
        "    except Exception:\n",
        "        target_margin = 0.0\n",
        "    board_obj = Board(sz)\n",
        "    history_boards = [board_obj.board.copy().astype(np.float32)]\n",
        "    current_player = 1\n",
        "    for node in sgf_data[\"nodes\"]:\n",
        "        move_prop = b'B' if current_player==1 else b'W'\n",
        "        move_vals = node.get(move_prop)\n",
        "        input_tensor = build_input_from_history(history_boards, current_player, sz, history_length)\n",
        "        if move_vals is None or len(move_vals)==0 or move_vals[0]==b\"\":\n",
        "            target_policy = np.zeros(sz*sz+1, dtype=np.float32)\n",
        "            target_policy[sz*sz] = 1.0\n",
        "        else:\n",
        "            try:\n",
        "                move = move_vals[0]\n",
        "                col = ord(move.decode('utf-8')[0])-ord('a')\n",
        "                row = ord(move.decode('utf-8')[1])-ord('a')\n",
        "                target_policy = np.zeros(sz*sz+1, dtype=np.float32)\n",
        "                target_policy[row*sz+col] = 1.0\n",
        "            except Exception as e:\n",
        "                sgf_logger.warning(f\"Error parsing move in SGF text: {e}\")\n",
        "                target_policy = np.zeros(sz*sz+1, dtype=np.float32)\n",
        "                target_policy[sz*sz] = 1.0\n",
        "        transforms = range(8) if augment_all else [np.random.randint(0,8)]\n",
        "        for t in transforms:\n",
        "            inp = apply_dihedral_transform(input_tensor, t)\n",
        "            pol = transform_policy(target_policy, t, sz)\n",
        "            samples.append((\n",
        "                inp.flatten(),\n",
        "                pol,\n",
        "                np.array([target_value], dtype=np.float32),\n",
        "                np.array([target_margin], dtype=np.float32)\n",
        "            ))\n",
        "        if move_vals is not None and len(move_vals)>0 and move_vals[0]!=b\"\":\n",
        "            try:\n",
        "                move = move_vals[0]\n",
        "                col = ord(move.decode('utf-8')[0])-ord('a')\n",
        "                row = ord(move.decode('utf-8')[1])-ord('a')\n",
        "                board_obj.play((row, col), 'b' if current_player==1 else 'w')\n",
        "                history_boards.append(board_obj.board.copy().astype(np.float32))\n",
        "            except Exception as e:\n",
        "                sgf_logger.warning(f\"Error updating board from SGF text: {e}\")\n",
        "        current_player = 2 if current_player==1 else 1\n",
        "    return samples\n",
        "\n",
        "# ==============================\n",
        "# データセットの保存／読み込み\n",
        "# ==============================\n",
        "def save_dataset(samples, output_file):\n",
        "    with open(output_file, \"wb\") as f:\n",
        "        pickle.dump(samples, f)\n",
        "    sgf_logger.info(f\"Saved dataset to {output_file}\")\n",
        "\n",
        "def load_dataset(output_file):\n",
        "    with open(output_file, \"rb\") as f:\n",
        "        samples = pickle.load(f)\n",
        "    sgf_logger.info(f\"Loaded dataset from {output_file}\")\n",
        "    return samples\n",
        "\n",
        "# ==============================\n",
        "# Test用データセット生成（zip利用）\n",
        "# ==============================\n",
        "def prepare_test_dataset(sgf_dir, board_size, history_length, augment_all, output_file):\n",
        "    if os.path.exists(output_file):\n",
        "        sgf_logger.info(f\"Test dataset pickle {output_file} already exists. Loading it directly...\")\n",
        "        return load_dataset(output_file)\n",
        "    if not os.path.exists(TEST_SGFS_ZIP):\n",
        "        sgf_logger.info(f\"Creating zip archive {TEST_SGFS_ZIP} from SGF files in {sgf_dir} ...\")\n",
        "        sgf_files = [os.path.join(sgf_dir, f) for f in os.listdir(sgf_dir)\n",
        "                     if f.endswith('.sgf') and \"analyzed\" not in f.lower()]\n",
        "        with zipfile.ZipFile(TEST_SGFS_ZIP, 'w') as zf:\n",
        "            for filepath in sgf_files:\n",
        "                zf.write(filepath, arcname=os.path.basename(filepath))\n",
        "        sgf_logger.info(f\"Zip archive created: {TEST_SGFS_ZIP}\")\n",
        "    else:\n",
        "        sgf_logger.info(f\"Zip archive {TEST_SGFS_ZIP} already exists. Loading from it...\")\n",
        "    all_samples = []\n",
        "    with zipfile.ZipFile(TEST_SGFS_ZIP, 'r') as zf:\n",
        "        sgf_names = [name for name in zf.namelist() if name.endswith('.sgf') and \"analyzed\" not in name.lower()]\n",
        "        sgf_names.sort()\n",
        "        sgf_logger.info(f\"TEST: Total SGF files in zip to process: {len(sgf_names)}\")\n",
        "        for name in tqdm(sgf_names, desc=\"Processing TEST SGF files\"):\n",
        "            try:\n",
        "                sgf_src = zf.read(name).decode('utf-8')\n",
        "                file_samples = process_sgf_to_samples_from_text(sgf_src, board_size, history_length, augment_all=False)\n",
        "                all_samples.extend(file_samples)\n",
        "            except Exception as e:\n",
        "                sgf_logger.error(f\"Error processing {name} from zip: {e}\")\n",
        "    save_dataset(all_samples, output_file)\n",
        "    sgf_logger.info(f\"TEST: Saved test dataset (total samples: {len(all_samples)}) to {output_file}\")\n",
        "    return all_samples\n",
        "\n",
        "# ==============================\n",
        "# グローバル変数：未処理のSGFファイルリスト\n",
        "# ==============================\n",
        "remaining_sgf_files = []\n",
        "\n",
        "def prepare_train_dataset_cycle(sgf_dir, board_size, history_length, augment_all, max_files):\n",
        "    \"\"\"\n",
        "    指定フォルダ内のSGFファイル（\"analyzed\"を含まない）全体から、\n",
        "    1サイクル分、ランダム順（重複なし）でmax_files件分のみ処理し、\n",
        "    各SGFから前処理済みサンプルを生成して返す。\n",
        "    \"\"\"\n",
        "    global remaining_sgf_files\n",
        "    if not remaining_sgf_files:\n",
        "        all_files = [os.path.join(sgf_dir, f) for f in os.listdir(sgf_dir)\n",
        "                     if f.endswith('.sgf') and \"analyzed\" not in f.lower()]\n",
        "        random.shuffle(all_files)\n",
        "        remaining_sgf_files = all_files\n",
        "        sgf_logger.info(\"Regenerated the random order of all SGF files.\")\n",
        "    if len(remaining_sgf_files) < max_files:\n",
        "        selected_files = remaining_sgf_files\n",
        "        remaining_sgf_files = []\n",
        "        sgf_logger.info(f\"Remaining SGF files less than max_files ({max_files}). Processing {len(selected_files)} files.\")\n",
        "    else:\n",
        "        selected_files = remaining_sgf_files[:max_files]\n",
        "        remaining_sgf_files = remaining_sgf_files[max_files:]\n",
        "        sgf_logger.info(f\"Selected {len(selected_files)} SGF files.\")\n",
        "    all_samples = []\n",
        "    for sgf_file in selected_files:\n",
        "        try:\n",
        "            with open(sgf_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                sgf_src = f.read()\n",
        "            file_samples = process_sgf_to_samples_from_text(sgf_src, board_size, history_length, augment_all)\n",
        "            all_samples.extend(file_samples)\n",
        "        except Exception as e:\n",
        "            sgf_logger.error(f\"Error processing file {sgf_file}: {e}\")\n",
        "    random.shuffle(all_samples)\n",
        "    sgf_logger.info(f\"Training dataset cycle created. Total samples: {len(all_samples)}\")\n",
        "    return all_samples\n",
        "\n",
        "def load_training_dataset(sgf_dir, board_size, history_length, augment_all, max_files):\n",
        "    \"\"\"\n",
        "    指定フォルダ内のSGFファイルから、max_files 件分のサンプルを一度だけ生成し、\n",
        "    前処理済みデータセット（AlphaZeroSGFDatasetPreloaded のインスタンス）を返す。\n",
        "    \"\"\"\n",
        "    samples = prepare_train_dataset_cycle(sgf_dir, board_size, history_length, augment_all, max_files)\n",
        "    dataset = AlphaZeroSGFDatasetPreloaded(samples)\n",
        "    return dataset\n",
        "\n",
        "# ==============================\n",
        "# 訓練ループ用関数（1エポック分）\n",
        "# ==============================\n",
        "def train_one_iteration(model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_policy_loss = 0.0\n",
        "    total_value_loss = 0.0\n",
        "    total_margin_loss = 0.0\n",
        "    num_batches = 0\n",
        "    overall_correct = 0\n",
        "    overall_samples = 0\n",
        "    value_loss_coefficient = 0.1\n",
        "    margin_loss_coefficient = 0.0001\n",
        "    print_interval = 100\n",
        "    accumulated_accuracy = 0.0\n",
        "    group_batches = 0\n",
        "    for boards, target_policies, target_values, target_margins in tqdm(train_loader, desc=\"Training\", bar_format=bar_fmt):\n",
        "        boards = boards.to(device)\n",
        "        target_policies = target_policies.to(device)\n",
        "        target_values = target_values.to(device)\n",
        "        target_margins = target_margins.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred_policy, (pred_value, pred_margin) = model(boards)\n",
        "        policy_loss = -torch.sum(target_policies * pred_policy) / boards.size(0)\n",
        "        value_loss = F.mse_loss(pred_value.view(-1), target_values.view(-1))\n",
        "        margin_loss = F.mse_loss(pred_margin.view(-1), target_margins.view(-1))\n",
        "        loss = policy_loss + value_loss_coefficient * value_loss + margin_loss_coefficient * margin_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if USE_TPU:\n",
        "            xm.mark_step()\n",
        "        total_loss += loss.item()\n",
        "        total_policy_loss += policy_loss.item()\n",
        "        total_value_loss += value_loss.item()\n",
        "        total_margin_loss += margin_loss.item()\n",
        "        num_batches += 1\n",
        "        batch_pred = pred_policy.argmax(dim=1)\n",
        "        batch_target = target_policies.argmax(dim=1)\n",
        "        batch_accuracy = (batch_pred == batch_target).float().mean().item()\n",
        "        overall_correct += (batch_pred == batch_target).sum().item()\n",
        "        overall_samples += boards.size(0)\n",
        "        accumulated_accuracy += batch_accuracy\n",
        "        group_batches += 1\n",
        "        if num_batches % print_interval == 0:\n",
        "            avg_accuracy = accumulated_accuracy / group_batches\n",
        "            start_batch = num_batches - group_batches + 1\n",
        "            end_batch = num_batches\n",
        "            print(f\"Batch {start_batch:4d}～{end_batch:4d} policy accuracy average: {avg_accuracy:6.4f}\")\n",
        "            accumulated_accuracy = 0.0\n",
        "            group_batches = 0\n",
        "        del boards, target_policies, target_values, target_margins\n",
        "    if group_batches > 0:\n",
        "        avg_accuracy = accumulated_accuracy / group_batches\n",
        "        print(f\"Other ({group_batches} batch) policy accuracy average: {avg_accuracy:6.4f}\")\n",
        "    if overall_samples > 0:\n",
        "        overall_accuracy = overall_correct / overall_samples\n",
        "        print(f\"Overall policy accuracy of the latest model state in this training loop: {overall_accuracy:6.4f}\")\n",
        "    else:\n",
        "        overall_accuracy = 0.0\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_policy_loss = total_policy_loss / num_batches\n",
        "    avg_value_loss = value_loss_coefficient * total_value_loss / num_batches\n",
        "    avg_margin_loss = margin_loss_coefficient * total_margin_loss / num_batches\n",
        "    train_logger.info(f\"Training iteration total average loss: {avg_loss:.5f}\")\n",
        "    train_logger.info(f\"Training iteration average policy loss: {avg_policy_loss:.5f}\")\n",
        "    train_logger.info(f\"Training iteration average value loss: {avg_value_loss:.5f}\")\n",
        "    train_logger.info(f\"Training iteration average margin loss: {avg_margin_loss:.5f}\")\n",
        "    train_logger.info(f\"Training iteration overall policy accuracy: {overall_accuracy:.5f}\")\n",
        "    return avg_loss\n",
        "\n",
        "# ==============================\n",
        "# チェックポイント保存＆復元\n",
        "# ==============================\n",
        "def save_checkpoint(model, optimizer, epoch, best_val_loss, epochs_no_improve, best_policy_accuracy, checkpoint_file, device):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'epochs_no_improve': epochs_no_improve,\n",
        "        'best_policy_accuracy': best_policy_accuracy\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_file)\n",
        "    train_logger.info(f\"Checkpoint saved at epoch {epoch} to {checkpoint_file}\")\n",
        "\n",
        "def recursive_to(data, device):\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        return data.to(device)\n",
        "    elif isinstance(data, dict):\n",
        "        return {k: recursive_to(v, device) for k, v in data.items()}\n",
        "    elif isinstance(data, list):\n",
        "        return [recursive_to(item, device) for item in data]\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "def load_checkpoint(model, optimizer, checkpoint_file, device):\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        checkpoint = torch.load(checkpoint_file, map_location=torch.device(\"cpu\"))\n",
        "        new_state_dict = {k: v.to(device) for k, v in checkpoint['model_state_dict'].items()}\n",
        "        model.load_state_dict(new_state_dict)\n",
        "        optimizer_state = recursive_to(checkpoint['optimizer_state_dict'], device)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "        epoch = checkpoint['epoch']\n",
        "        best_policy_accuracy = checkpoint.get('best_policy_accuracy', 0.0)\n",
        "        train_logger.info(f\"Checkpoint loaded from {checkpoint_file} at epoch {epoch}\")\n",
        "        return epoch, best_policy_accuracy\n",
        "    else:\n",
        "        train_logger.info(\"No checkpoint found. Starting from scratch.\")\n",
        "        return 0, 0.0\n",
        "\n",
        "# ==============================\n",
        "# メイン用：訓練用データセットのキャッシュ読み込み\n",
        "# ==============================\n",
        "def load_training_dataset(sgf_dir, board_size, history_length, augment_all, max_files):\n",
        "    samples = prepare_train_dataset_cycle(sgf_dir, board_size, history_length, augment_all, max_files)\n",
        "    dataset = AlphaZeroSGFDatasetPreloaded(samples)\n",
        "    return dataset\n",
        "\n",
        "# ==============================\n",
        "# TPU分散環境で動作するメイン処理\n",
        "# ==============================\n",
        "def _mp_fn(rank):\n",
        "    if USE_TPU:\n",
        "        if not dist.is_initialized():\n",
        "            dist.init_process_group(\"xla\", init_method='xla://')\n",
        "        device = xm.xla_device()\n",
        "        train_logger.info(\"Running on TPU device: {}\".format(device))\n",
        "    else:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        train_logger.info(\"Running on device: {}\".format(device))\n",
        "    test_dataset_pickle = os.path.join(VAL_SGF_DIR, \"test_dataset.pkl\")\n",
        "    test_samples = prepare_test_dataset(VAL_SGF_DIR, BOARD_SIZE, HISTORY_LENGTH, True, test_dataset_pickle)\n",
        "    test_dataset = AlphaZeroSGFDatasetPreloaded(test_samples)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    model = EnhancedResNetPolicyValueNetwork(\n",
        "        board_size=BOARD_SIZE,\n",
        "        num_channels=model_channels,\n",
        "        num_residual_blocks=num_residual_blocks,\n",
        "        in_channels=NUM_CHANNELS\n",
        "    ).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience, factor=factor)\n",
        "    start_epoch, best_policy_accuracy = load_checkpoint(model, optimizer, CHECKPOINT_FILE, device)\n",
        "    for f in os.listdir(MODEL_OUTPUT_DIR):\n",
        "        if f.startswith(\"model_\") and f.endswith(\".pt\"):\n",
        "            try:\n",
        "                acc = float(f[len(\"model_\"):-len(\".pt\")])\n",
        "                if acc > best_policy_accuracy:\n",
        "                    best_policy_accuracy = acc\n",
        "                    best_model_file = os.path.join(MODEL_OUTPUT_DIR, f)\n",
        "                    model.load_state_dict(torch.load(best_model_file, map_location=device))\n",
        "                    train_logger.info(\"Restored best model with policy accuracy {:.5f} from {}\".format(acc, best_model_file))\n",
        "            except Exception:\n",
        "                continue\n",
        "    train_logger.info(\"Initial best_policy_accuracy: {:.5f}\".format(best_policy_accuracy))\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    train_logger.info(\"Current learning rate : {:.8f}\".format(current_lr))\n",
        "    training_dataset = load_training_dataset(TRAIN_SGF_DIR, BOARD_SIZE, HISTORY_LENGTH, augment_all=True, max_files=number_max_files)\n",
        "    epoch = start_epoch\n",
        "    while True:\n",
        "        train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
        "        train_one_iteration(model, train_loader, optimizer, device)\n",
        "        epoch += 1\n",
        "        policy_accuracy = validate_model(model, test_loader, device)\n",
        "        if policy_accuracy > best_policy_accuracy:\n",
        "            best_policy_accuracy = save_best_model(model, policy_accuracy, device, best_policy_accuracy)\n",
        "        else:\n",
        "            save_inference_model(model, device, \"inference2_model_tmp.pt\")\n",
        "        lr_before = optimizer.param_groups[0]['lr']\n",
        "        train_logger.info(\"Epoch {} - Before scheduler.step(): lr = {:.8f}\".format(epoch+1, lr_before))\n",
        "        scheduler.step(policy_accuracy)\n",
        "        lr_after = optimizer.param_groups[0]['lr']\n",
        "        train_logger.info(\"Epoch {} - After scheduler.step(): lr = {:.8f}\".format(epoch+1, lr_after))\n",
        "        dummy_best_val_loss = 0.0\n",
        "        dummy_epochs_no_improve = 0\n",
        "        save_checkpoint(model, optimizer, epoch, dummy_best_val_loss, dummy_epochs_no_improve, best_policy_accuracy, CHECKPOINT_FILE, device)\n",
        "        train_logger.info(\"Iteration completed. Restarting next iteration...\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--config\", type=str, default=os.path.join(BASE_DIR, \"config_py.ini\"),\n",
        "                        help=\"Path to configuration file\")\n",
        "    parser.add_argument(\"--checkpoint\", type=str, default=CHECKPOINT_FILE,\n",
        "                        help=\"Path to checkpoint file\")\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    if not os.path.exists(args.config):\n",
        "        sgf_logger.warning(\"Config file not found. Using default hyperparameters.\")\n",
        "    train_logger.info(\"=== Starting Training and Validation Loop ===\")\n",
        "    if USE_TPU:\n",
        "        import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "        nprocs = 1\n",
        "        xmp.spawn(_mp_fn, args=(), nprocs=nprocs)\n",
        "    else:\n",
        "        _mp_fn(0)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyN0PFFIA3wqkyIgUFdfDNYf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}