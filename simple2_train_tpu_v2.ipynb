{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cafeblue999/test/blob/master/simple2_train_tpu_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAQL60g2jIcK",
        "outputId": "3d186c02-5b80-4213-f855-3a67fe79da34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "2025-04-13 16:05:39 INFO: ==== Loaded Configuration ====\n",
            "2025-04-13 16:05:39 INFO: Config file: /content/drive/My Drive/sgf/config_py.ini\n",
            "2025-04-13 16:05:39 INFO: BOARD_SIZE: 19\n",
            "2025-04-13 16:05:39 INFO: HISTORY_LENGTH: 8\n",
            "2025-04-13 16:05:39 INFO: NUM_CHANNELS: 17\n",
            "2025-04-13 16:05:39 INFO: NUM_ACTIONS: 362\n",
            "2025-04-13 16:05:39 INFO: num_residual_blocks: 20\n",
            "2025-04-13 16:05:39 INFO: model_channels: 256\n",
            "2025-04-13 16:05:39 INFO: num_epochs: 1000\n",
            "2025-04-13 16:05:39 INFO: batch_size: 256\n",
            "2025-04-13 16:05:39 INFO: learning_rate: 0.001\n",
            "2025-04-13 16:05:39 INFO: patience: 1\n",
            "2025-04-13 16:05:39 INFO: factor: 0.8\n",
            "2025-04-13 16:05:39 INFO: number_max_files: 256\n",
            "2025-04-13 16:05:39 INFO: ===============================\n",
            "2025-04-13 16:05:39 INFO: === Starting Training and Validation Loop ===\n",
            "2025-04-13 16:05:39 INFO: Running on TPU device: xla:0\n",
            "2025-04-13 16:05:40 INFO: Test dataset pickle /content/drive/My Drive/sgf/test/test_dataset.pkl already exists. Loading it directly...\n",
            "2025-04-13 16:07:12 INFO: Loaded dataset from /content/drive/My Drive/sgf/test/test_dataset.pkl\n",
            "2025-04-13 16:07:13 INFO: No checkpoint found. Starting from scratch.\n",
            "2025-04-13 16:07:13 INFO: Initial best_policy_accuracy: 0.00000\n",
            "2025-04-13 16:07:13 INFO: Current learning rate : 0.00100000\n",
            "2025-04-13 16:07:13 INFO: Regenerated the random order of all SGF files.\n",
            "2025-04-13 16:07:13 INFO: Selected 256 SGF files.\n",
            "2025-04-13 16:09:46 INFO: Training dataset cycle created. Total samples: 642280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|▍         |    100/  2509 [01:50<36:18,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch    1～ 100 policy accuracy average: 0.0074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|▊         |    200/  2509 [03:19<34:59,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  101～ 200 policy accuracy average: 0.0129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  12%|█▏        |    300/  2509 [04:49<33:13,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  201～ 300 policy accuracy average: 0.0146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  16%|█▌        |    400/  2509 [06:20<31:41,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  301～ 400 policy accuracy average: 0.0167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|█▉        |    500/  2509 [07:50<30:14,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  401～ 500 policy accuracy average: 0.0229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|██▍       |    600/  2509 [09:20<28:41,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  501～ 600 policy accuracy average: 0.0304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|██▊       |    700/  2509 [10:51<27:14,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  601～ 700 policy accuracy average: 0.0347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  32%|███▏      |    800/  2509 [12:21<25:43,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  701～ 800 policy accuracy average: 0.0426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  36%|███▌      |    900/  2509 [13:52<24:21,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  801～ 900 policy accuracy average: 0.0460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  40%|███▉      |   1000/  2509 [15:23<22:36,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  901～1000 policy accuracy average: 0.0511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  44%|████▍     |   1100/  2509 [16:53<21:15,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1001～1100 policy accuracy average: 0.0548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  48%|████▊     |   1200/  2509 [18:24<19:44,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1101～1200 policy accuracy average: 0.0609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  52%|█████▏    |   1300/  2509 [19:54<17:39,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1201～1300 policy accuracy average: 0.0720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  56%|█████▌    |   1400/  2509 [21:22<16:42,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1301～1400 policy accuracy average: 0.0779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  60%|█████▉    |   1500/  2509 [22:53<14:53,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1401～1500 policy accuracy average: 0.0783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  64%|██████▍   |   1600/  2509 [24:22<13:42,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1501～1600 policy accuracy average: 0.0771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  68%|██████▊   |   1700/  2509 [25:50<11:52,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1601～1700 policy accuracy average: 0.0891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  72%|███████▏  |   1800/  2509 [27:21<10:42,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1701～1800 policy accuracy average: 0.0922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  76%|███████▌  |   1900/  2509 [28:51<09:11,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1801～1900 policy accuracy average: 0.0989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  80%|███████▉  |   2000/  2509 [30:21<07:39,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1901～2000 policy accuracy average: 0.1021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  84%|████████▎ |   2100/  2509 [31:52<06:03,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2001～2100 policy accuracy average: 0.1032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  88%|████████▊ |   2200/  2509 [33:22<04:38,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2101～2200 policy accuracy average: 0.0970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  92%|█████████▏|   2300/  2509 [34:52<03:09,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2201～2300 policy accuracy average: 0.1008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  96%|█████████▌|   2400/  2509 [36:22<01:38,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2301～2400 policy accuracy average: 0.1007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|█████████▉|   2500/  2509 [37:53<00:08,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2401～2500 policy accuracy average: 0.0811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████|   2509/  2509 [38:14<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Other (9 batch) policy accuracy average: 0.1003\n",
            "Overall policy accuracy of the latest model state in this training loop: 0.0627\n",
            "2025-04-13 16:48:00 INFO: Training iteration total average loss: 5.00126\n",
            "2025-04-13 16:48:00 INFO: Training iteration average policy loss: 4.95083\n",
            "2025-04-13 16:48:00 INFO: Training iteration average value loss: 0.04295\n",
            "2025-04-13 16:48:00 INFO: Training iteration average margin loss: 0.00747\n",
            "2025-04-13 16:48:00 INFO: Training iteration overall policy accuracy: 0.06274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████|   1967/  1967 [08:03<00:00,  4.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-13 16:56:04 INFO: ===== Validation Policy Accuracy ==== 【0.10632】\n",
            "2025-04-13 16:56:05 INFO: ● New best model saved (state_dict): /content/drive/My Drive/sgf/models/model_0.10632.pt\n",
            "2025-04-13 16:56:06 INFO: Inference model saved as /content/drive/My Drive/sgf/models/inference2_model.pt\n",
            "2025-04-13 16:56:06 INFO: Epoch 2 - Before scheduler.step(): lr = 0.00100000\n",
            "2025-04-13 16:56:06 INFO: Epoch 2 - After scheduler.step(): lr = 0.00100000\n",
            "2025-04-13 16:56:07 INFO: Checkpoint saved at epoch 1 to /content/drive/My Drive/sgf/checkpoint2.pt\n",
            "2025-04-13 16:56:07 INFO: Iteration completed. Restarting next iteration...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|▍         |    100/  2509 [01:31<30:17,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch    1～ 100 policy accuracy average: 0.0996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|▊         |    200/  2509 [02:46<28:56,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  101～ 200 policy accuracy average: 0.0993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  12%|█▏        |    300/  2509 [04:01<27:38,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  201～ 300 policy accuracy average: 0.1002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  16%|█▌        |    400/  2509 [05:17<26:34,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  301～ 400 policy accuracy average: 0.1010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|█▉        |    500/  2509 [06:32<25:12,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  401～ 500 policy accuracy average: 0.1014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|██▍       |    600/  2509 [07:47<23:54,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  501～ 600 policy accuracy average: 0.1047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|██▊       |    700/  2509 [09:03<22:36,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  601～ 700 policy accuracy average: 0.0996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  32%|███▏      |    800/  2509 [10:18<21:27,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  701～ 800 policy accuracy average: 0.1008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  36%|███▌      |    900/  2509 [11:33<20:11,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  801～ 900 policy accuracy average: 0.1018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  40%|███▉      |   1000/  2509 [12:49<18:56,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  901～1000 policy accuracy average: 0.1022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  44%|████▍     |   1100/  2509 [14:04<17:46,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1001～1100 policy accuracy average: 0.1044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  48%|████▊     |   1200/  2509 [15:20<16:27,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1101～1200 policy accuracy average: 0.1017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  52%|█████▏    |   1300/  2509 [16:35<15:08,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1201～1300 policy accuracy average: 0.0972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  56%|█████▌    |   1400/  2509 [17:50<13:53,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1301～1400 policy accuracy average: 0.1002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  60%|█████▉    |   1500/  2509 [19:06<12:40,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1401～1500 policy accuracy average: 0.1045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  64%|██████▍   |   1600/  2509 [20:21<11:24,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1501～1600 policy accuracy average: 0.1014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  68%|██████▊   |   1700/  2509 [21:37<10:10,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1601～1700 policy accuracy average: 0.1006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  72%|███████▏  |   1800/  2509 [22:52<08:58,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1701～1800 policy accuracy average: 0.1036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  76%|███████▌  |   1900/  2509 [24:07<07:40,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1801～1900 policy accuracy average: 0.1020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  80%|███████▉  |   2000/  2509 [25:23<06:23,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1901～2000 policy accuracy average: 0.1011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  84%|████████▎ |   2100/  2509 [26:38<05:07,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2001～2100 policy accuracy average: 0.1036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  88%|████████▊ |   2200/  2509 [27:54<03:53,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2101～2200 policy accuracy average: 0.1012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  92%|█████████▏|   2300/  2509 [29:09<02:37,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2201～2300 policy accuracy average: 0.1014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  96%|█████████▌|   2400/  2509 [30:25<01:21,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2301～2400 policy accuracy average: 0.1005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|█████████▉|   2500/  2509 [31:40<00:06,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2401～2500 policy accuracy average: 0.0979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████|   2509/  2509 [31:58<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Other (9 batch) policy accuracy average: 0.1069\n",
            "Overall policy accuracy of the latest model state in this training loop: 0.1013\n",
            "2025-04-13 17:28:06 INFO: Training iteration total average loss: 4.60704\n",
            "2025-04-13 17:28:06 INFO: Training iteration average policy loss: 4.57350\n",
            "2025-04-13 17:28:06 INFO: Training iteration average value loss: 0.02788\n",
            "2025-04-13 17:28:06 INFO: Training iteration average margin loss: 0.00565\n",
            "2025-04-13 17:28:06 INFO: Training iteration overall policy accuracy: 0.10130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████|   1967/  1967 [07:56<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-13 17:36:02 INFO: ===== Validation Policy Accuracy ==== 【0.11554】\n",
            "2025-04-13 17:36:02 INFO: ● New best model saved (state_dict): /content/drive/My Drive/sgf/models/model_0.11554.pt\n",
            "2025-04-13 17:36:04 INFO: Inference model saved as /content/drive/My Drive/sgf/models/inference2_model.pt\n",
            "2025-04-13 17:36:04 INFO: Deleted old model: model_0.10632.pt\n",
            "2025-04-13 17:36:04 INFO: Epoch 3 - Before scheduler.step(): lr = 0.00100000\n",
            "2025-04-13 17:36:04 INFO: Epoch 3 - After scheduler.step(): lr = 0.00100000\n",
            "2025-04-13 17:36:05 INFO: Checkpoint saved at epoch 2 to /content/drive/My Drive/sgf/checkpoint2.pt\n",
            "2025-04-13 17:36:05 INFO: Iteration completed. Restarting next iteration...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|▍         |    100/  2509 [01:15<30:10,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch    1～ 100 policy accuracy average: 0.0995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|▊         |    200/  2509 [02:30<28:57,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  101～ 200 policy accuracy average: 0.1002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  12%|█▏        |    300/  2509 [03:45<27:32,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  201～ 300 policy accuracy average: 0.1004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  16%|█▌        |    400/  2509 [05:00<26:21,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  301～ 400 policy accuracy average: 0.1023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|█▉        |    500/  2509 [06:15<25:14,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  401～ 500 policy accuracy average: 0.1002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|██▍       |    600/  2509 [07:30<23:54,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  501～ 600 policy accuracy average: 0.1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|██▊       |    700/  2509 [08:46<22:35,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  601～ 700 policy accuracy average: 0.1013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  32%|███▏      |    800/  2509 [10:01<21:21,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  701～ 800 policy accuracy average: 0.1008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  36%|███▌      |    900/  2509 [11:16<20:18,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  801～ 900 policy accuracy average: 0.1010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  40%|███▉      |   1000/  2509 [12:31<18:50,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  901～1000 policy accuracy average: 0.1018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  44%|████▍     |   1100/  2509 [13:46<17:35,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1001～1100 policy accuracy average: 0.1020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  48%|████▊     |   1200/  2509 [15:01<16:26,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1101～1200 policy accuracy average: 0.1020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  52%|█████▏    |   1300/  2509 [16:16<15:09,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1201～1300 policy accuracy average: 0.1001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  56%|█████▌    |   1400/  2509 [17:32<13:51,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1301～1400 policy accuracy average: 0.1020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  57%|█████▋    |   1436/  2509 [17:59<13:29,  1.32it/s]"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "train.py\n",
        "(詳細な説明は省略されていますが、全体としては囲碁棋譜（SGF）のパース、前処理、\n",
        "ディープラーニングモデルの定義、学習ループ、チェックポイント保存・復元、\n",
        "およびTPUやGoogle Colabを利用する際の環境設定を行うコード)\n",
        "\"\"\"\n",
        "\n",
        "# ===== 固定定義：環境切り替え用フラグ =====\n",
        "# TPU、Colabで実行するかどうかのフラグを定義\n",
        "USE_TPU = True\n",
        "USE_COLAB = True\n",
        "\n",
        "# ------------------------------\n",
        "# 必要なライブラリのインポート\n",
        "# ------------------------------\n",
        "import os, re, pickle, zipfile, random, numpy as np, configparser, argparse, functools\n",
        "from tqdm import tqdm  # 進捗表示用ライブラリ\n",
        "import torch, torch.nn as nn, torch.optim as optim, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.distributed as dist  # 分散学習用\n",
        "\n",
        "# TPU利用時に必要なtorch_xla関連モジュールをインポート\n",
        "if USE_TPU:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    import torch_xla.distributed.xla_backend\n",
        "\n",
        "# Google Colab利用時にGoogle Driveのマウントを試みる\n",
        "if USE_COLAB:\n",
        "    try:\n",
        "        # 既にDriveがマウントされている場合の解除\n",
        "        os.system(\"fusermount -u /content/drive\")\n",
        "    except Exception as e:\n",
        "        print(\"Google Drive unmount failed:\", e)\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "    except ImportError:\n",
        "        print(\"Google Colab module not found.\")\n",
        "\n",
        "# プログレスバーのフォーマット設定\n",
        "bar_fmt = \"{l_bar}{bar}| {n:>6d}/{total:>6d} [{elapsed}<{remaining}, {rate_fmt}]\"\n",
        "\n",
        "# ==============================\n",
        "# デバイス設定\n",
        "# ==============================\n",
        "if USE_TPU:\n",
        "    # TPUデバイスの取得\n",
        "    device = xm.xla_device()\n",
        "    # 分散処理が初期化されていなければ初期化\n",
        "    if not dist.is_initialized():\n",
        "        dist.init_process_group(\"xla\", init_method='xla://')\n",
        "else:\n",
        "    # GPUが利用可能であればCUDA、なければCPUを使用\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==============================\n",
        "# ディレクトリ設定\n",
        "# ==============================\n",
        "if USE_COLAB:\n",
        "    # Colab環境用のディレクトリ設定\n",
        "    BASE_DIR = \"/content/drive/My Drive/sgf\"\n",
        "    TRAIN_SGF_DIR = os.path.join(BASE_DIR, \"train_sgf_KK\")\n",
        "    VAL_SGF_DIR = os.path.join(BASE_DIR, \"test\")\n",
        "    TEST_SGFS_ZIP = os.path.join(VAL_SGF_DIR, \"test_sgfs.zip\")\n",
        "    MODEL_OUTPUT_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "    CHECKPOINT_FILE = os.path.join(BASE_DIR, \"checkpoint2.pt\")\n",
        "else:\n",
        "    # ローカル環境用のディレクトリ設定（例：Windows環境）\n",
        "    BASE_DIR = r\"D:\\igo\\simple2\"\n",
        "    TRAIN_SGF_DIR = os.path.join(BASE_DIR, \"train_sgf\")\n",
        "    VAL_SGF_DIR = os.path.join(BASE_DIR, \"test\")\n",
        "    TEST_SGFS_ZIP = os.path.join(VAL_SGF_DIR, \"test_sgfs.zip\")\n",
        "    MODEL_OUTPUT_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "    CHECKPOINT_FILE = os.path.join(BASE_DIR, \"checkpoint2.pt\")\n",
        "\n",
        "# モデル出力用ディレクトリが存在しなければ作成\n",
        "if not os.path.exists(MODEL_OUTPUT_DIR):\n",
        "    os.makedirs(MODEL_OUTPUT_DIR)\n",
        "\n",
        "# ==============================\n",
        "# DummyLogger クラス（ログ出力用）\n",
        "# ==============================\n",
        "from datetime import datetime, timedelta, timezone\n",
        "# 日本標準時(JST)のタイムゾーン設定\n",
        "JST = timezone(timedelta(hours=9), 'JST')\n",
        "class DummyLogger:\n",
        "    # infoレベルのログ出力：タイムスタンプ付き\n",
        "    def info(self, message, *args, **kwargs):\n",
        "        timestamp = datetime.now(JST).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"{timestamp} INFO: {message}\", *args, **kwargs)\n",
        "    # warningレベルのログ出力\n",
        "    def warning(self, message, *args, **kwargs):\n",
        "        timestamp = datetime.now(JST).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"{timestamp} WARNING: {message}\", *args, **kwargs)\n",
        "    # errorレベルのログ出力\n",
        "    def error(self, message, *args, **kwargs):\n",
        "        timestamp = datetime.now(JST).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"{timestamp} ERROR: {message}\", *args, **kwargs)\n",
        "\n",
        "# ロガーインスタンスの生成\n",
        "sgf_logger = DummyLogger()\n",
        "train_logger = DummyLogger()\n",
        "\n",
        "# ==============================\n",
        "# 設定ファイル読み込み関数\n",
        "# ==============================\n",
        "def load_config(config_path):\n",
        "    # 設定ファイルをパースするためのConfigParserの生成\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read(config_path)\n",
        "    try:\n",
        "        # 各セクションから必要なパラメータを取得（存在しない場合はfallback値を利用）\n",
        "        BOARD_SIZE = int(config.get(\"BOARD\", \"board_size\", fallback=\"19\"))\n",
        "        HISTORY_LENGTH = int(config.get(\"DATA\", \"history_length\", fallback=\"8\"))\n",
        "\n",
        "        # チャンネル数は履歴の2倍+現在のプレイヤー情報\n",
        "        NUM_CHANNELS = 2 * HISTORY_LENGTH + 1\n",
        "\n",
        "        # 全手数は盤上のマス＋パス（最後の1手）\n",
        "        NUM_ACTIONS = BOARD_SIZE * BOARD_SIZE + 1\n",
        "        num_residual_blocks = int(config.get(\"MODEL\", \"num_residual_blocks\", fallback=\"20\"))\n",
        "        model_channels = int(config.get(\"MODEL\", \"model_channels\", fallback=\"256\"))\n",
        "        num_epochs = int(config.get(\"TRAIN\", \"num_epochs\", fallback=\"100\"))\n",
        "        batch_size = int(config.get(\"TRAIN\", \"batch_size\", fallback=\"256\"))\n",
        "        learning_rate = float(config.get(\"TRAIN\", \"learning_rate\", fallback=\"0.001\"))\n",
        "        patience = int(config.get(\"TRAIN\", \"patience\", fallback=\"10\"))\n",
        "        factor = float(config.get(\"TRAIN\", \"factor\", fallback=\"0.8\"))\n",
        "        number_max_files = int(config.get(\"TRAIN\", \"number_max_files\", fallback=\"256\"))\n",
        "    except Exception as e:\n",
        "        train_logger.error(f\"Error reading configuration: {e}\")\n",
        "        exit(1)\n",
        "\n",
        "    # 辞書形式でパラメータを返す\n",
        "    return {\n",
        "        \"BOARD_SIZE\": BOARD_SIZE,\n",
        "        \"HISTORY_LENGTH\": HISTORY_LENGTH,\n",
        "        \"NUM_CHANNELS\": NUM_CHANNELS,\n",
        "        \"NUM_ACTIONS\": NUM_ACTIONS,\n",
        "        \"num_residual_blocks\": num_residual_blocks,\n",
        "        \"model_channels\": model_channels,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"patience\": patience,\n",
        "        \"factor\": factor,\n",
        "        \"number_max_files\": number_max_files\n",
        "    }\n",
        "\n",
        "# 設定ファイルのパス（BASE_DIR以下にあると仮定）\n",
        "CONFIG_PATH = os.path.join(BASE_DIR, \"config_py.ini\")\n",
        "# 設定をロードし、パラメータを辞書に格納\n",
        "config_params = load_config(CONFIG_PATH)\n",
        "\n",
        "# ロードしたパラメータをグローバル変数に代入\n",
        "BOARD_SIZE = config_params[\"BOARD_SIZE\"]\n",
        "HISTORY_LENGTH = config_params[\"HISTORY_LENGTH\"]\n",
        "NUM_CHANNELS = config_params[\"NUM_CHANNELS\"]\n",
        "NUM_ACTIONS = config_params[\"NUM_ACTIONS\"]\n",
        "num_residual_blocks = config_params[\"num_residual_blocks\"]\n",
        "model_channels = config_params[\"model_channels\"]\n",
        "num_epochs = config_params[\"num_epochs\"]\n",
        "batch_size = config_params[\"batch_size\"]\n",
        "learning_rate = config_params[\"learning_rate\"]\n",
        "patience = config_params[\"patience\"]\n",
        "factor = config_params[\"factor\"]\n",
        "number_max_files = config_params[\"number_max_files\"]\n",
        "\n",
        "# ログ出力：読み込んだ設定の確認\n",
        "train_logger.info(\"==== Loaded Configuration ====\")\n",
        "train_logger.info(f\"Config file: {CONFIG_PATH}\")\n",
        "train_logger.info(f\"BOARD_SIZE: {BOARD_SIZE}\")\n",
        "train_logger.info(f\"HISTORY_LENGTH: {HISTORY_LENGTH}\")\n",
        "train_logger.info(f\"NUM_CHANNELS: {NUM_CHANNELS}\")\n",
        "train_logger.info(f\"NUM_ACTIONS: {NUM_ACTIONS}\")\n",
        "train_logger.info(f\"num_residual_blocks: {num_residual_blocks}\")\n",
        "train_logger.info(f\"model_channels: {model_channels}\")\n",
        "train_logger.info(f\"num_epochs: {num_epochs}\")\n",
        "train_logger.info(f\"batch_size: {batch_size}\")\n",
        "train_logger.info(f\"learning_rate: {learning_rate}\")\n",
        "train_logger.info(f\"patience: {patience}\")\n",
        "train_logger.info(f\"factor: {factor}\")\n",
        "train_logger.info(f\"number_max_files: {number_max_files}\")\n",
        "train_logger.info(\"===============================\")\n",
        "\n",
        "# ==============================\n",
        "# ネットワーク定義\n",
        "# ==============================\n",
        "# ResidualBlock : 標準的な残差ブロック。入力と出力のテンソルのサイズは同じで、\n",
        "# 畳み込み→バッチ正規化→ReLUを2回適用し、最終的に入力と出力を足し合わせる。\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)  # 畳み込み層1\n",
        "        self.bn1 = nn.BatchNorm2d(channels)  # バッチ正規化1\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)  # 畳み込み層2\n",
        "        self.bn2 = nn.BatchNorm2d(channels)  # バッチ正規化2\n",
        "    def forward(self, x):\n",
        "        residual = x  # 入力を保持（スキップ接続）\n",
        "        out = F.relu(self.bn1(self.conv1(x)))  # 畳み込み→正規化→ReLU\n",
        "        out = self.bn2(self.conv2(out))  # 再度畳み込み→正規化\n",
        "        out += residual  # 入力と加算（残差接続）\n",
        "        return F.relu(out)  # 最後にReLU適用して出力\n",
        "\n",
        "# DilatedResidualBlock : 拡張畳み込みを用いた残差ブロック。\n",
        "class DilatedResidualBlock(nn.Module):\n",
        "    def __init__(self, channels, dilation=2):\n",
        "        super(DilatedResidualBlock, self).__init__()\n",
        "        # 拡張畳み込み（dilation指定）を適用\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=dilation, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        # 通常の畳み込み層\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += residual\n",
        "        return F.relu(out)\n",
        "\n",
        "# SelfAttention : セルフアテンション機構を実現する層。\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        # クエリ、キー、バリューの線形変換（1x1畳み込み）を定義\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))  # 出力のスケールパラメータ\n",
        "        self.softmax = nn.Softmax(dim=-1)  # ソフトマックス関数\n",
        "    def forward(self, x):\n",
        "        batch, C, H, W = x.size()\n",
        "        # クエリを計算し形状変換（バッチ×(H×W)×(C//8)）\n",
        "        proj_query = self.query_conv(x).view(batch, -1, H * W).permute(0, 2, 1)\n",
        "        # キーを計算し形状変換（バッチ×(C//8)×(H×W)）\n",
        "        proj_key = self.key_conv(x).view(batch, -1, H * W)\n",
        "        # バッチ行列積によりエネルギーを計算\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        # 各位置ごとに正規化（アテンションマップ）\n",
        "        attention = self.softmax(energy)\n",
        "        # バリューを計算して形状変換\n",
        "        proj_value = self.value_conv(x).view(batch, -1, H * W)\n",
        "        # バッチ行列積により出力を計算（アテンションの転置を適用）\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch, C, H, W)\n",
        "        # スケールパラメータを乗じ、入力を足して出力\n",
        "        return self.gamma * out + x\n",
        "\n",
        "# EnhancedResNetPolicyValueNetwork : 改良版残差ネットワークを基に\n",
        "# ポリシーヘッドとバリューヘッドを組み合わせたネットワーク\n",
        "class EnhancedResNetPolicyValueNetwork(nn.Module):\n",
        "    def __init__(self, board_size, num_channels, num_residual_blocks, in_channels):\n",
        "        super(EnhancedResNetPolicyValueNetwork, self).__init__()\n",
        "        self.board_size = board_size\n",
        "        # 入力処理：3x3畳み込みと正規化\n",
        "        self.conv_input = nn.Conv2d(in_channels, num_channels, kernel_size=3, padding=1)\n",
        "        self.bn_input = nn.BatchNorm2d(num_channels)\n",
        "        # 複数の残差ブロック（定期的に拡張畳み込みブロックを挿入）\n",
        "        blocks = []\n",
        "        for i in range(num_residual_blocks):\n",
        "            if i % 4 == 0:\n",
        "                blocks.append(DilatedResidualBlock(num_channels, dilation=2))\n",
        "            else:\n",
        "                blocks.append(ResidualBlock(num_channels))\n",
        "        self.residual_blocks = nn.Sequential(*blocks)\n",
        "        # セルフアテンション層\n",
        "        self.attention = SelfAttention(num_channels)\n",
        "        # ポリシーヘッド\n",
        "        self.conv_policy = nn.Conv2d(num_channels, 2, kernel_size=1)\n",
        "        self.bn_policy = nn.BatchNorm2d(2)\n",
        "        self.dropout_policy = nn.Dropout(p=0.5)\n",
        "        # 全結合層により盤面上の各マス＋パスに対応する出力（行動数）を得る\n",
        "        self.fc_policy = nn.Linear(2 * board_size * board_size, NUM_ACTIONS)\n",
        "        # バリューヘッド\n",
        "        self.conv_value = nn.Conv2d(num_channels, 1, kernel_size=1)\n",
        "        self.bn_value = nn.BatchNorm2d(1)\n",
        "        # 隠れ層の全結合\n",
        "        self.fc_value1 = nn.Linear(board_size * board_size, 64)\n",
        "        self.dropout_value = nn.Dropout(p=0.5)\n",
        "        # 出力層：2ユニット、1はvalue、もう1つはmargin\n",
        "        self.fc_value2 = nn.Linear(64, 2)\n",
        "    def forward(self, x):\n",
        "        # 入力を処理（畳み込み→正規化→ReLU）\n",
        "        x = F.relu(self.bn_input(self.conv_input(x)))\n",
        "        # 残差ブロックの適用\n",
        "        x = self.residual_blocks(x)\n",
        "        # セルフアテンションによる特徴変換\n",
        "        x = self.attention(x)\n",
        "        # ---- ポリシーヘッドの処理 ----\n",
        "        p = F.relu(self.bn_policy(self.conv_policy(x)))\n",
        "        p = self.dropout_policy(p)\n",
        "        p = p.view(p.size(0), -1)  # フラット化して全結合層へ\n",
        "        p = self.fc_policy(p)\n",
        "        p = F.log_softmax(p, dim=1)  # 対数ソフトマックスで正規化\n",
        "        # ---- バリューヘッドの処理 ----\n",
        "        v = F.relu(self.bn_value(self.conv_value(x)))\n",
        "        v = v.view(v.size(0), -1)\n",
        "        v = F.relu(self.fc_value1(v))\n",
        "        v = self.dropout_value(v)\n",
        "        out = self.fc_value2(v)\n",
        "        # 1ユニット目を[tanh]により[-1, 1]に収め、残りをmarginとして出力\n",
        "        value = torch.tanh(out[:, 0])\n",
        "        margin = out[:, 1]\n",
        "        return p, (value, margin)\n",
        "\n",
        "# ==============================\n",
        "# SGFパーサー＆前処理関数\n",
        "# ==============================\n",
        "def parse_sgf(sgf_text):\n",
        "    \"\"\"\n",
        "    SGF形式の文字列をパースして、棋譜のノード情報を抽出する。\n",
        "    ・先頭と末尾の括弧を除去\n",
        "    ・セミコロンでノードに分割し、各ノードのプロパティを正規表現で抽出\n",
        "    \"\"\"\n",
        "    sgf_text = sgf_text.strip()  # 前後の空白除去\n",
        "    if sgf_text.startswith('(') and sgf_text.endswith(')'):\n",
        "        sgf_text = sgf_text[1:-1]  # 外側の括弧を除去\n",
        "\n",
        "    # セミコロンで分割し、空でない部分のみ抽出\n",
        "    parts = [part for part in sgf_text.split(';') if part.strip()]\n",
        "    nodes = []\n",
        "\n",
        "    # プロパティのキーと値をマッチする正規表現パターン\n",
        "    prop_pattern = re.compile(r'([A-Z]+)\\[([^\\]]*)\\]')\n",
        "    for part in parts:\n",
        "        props = {}\n",
        "        # 正規表現でプロパティを抽出し、キーはUTF-8エンコード、値はリストに格納\n",
        "        for m in prop_pattern.finditer(part):\n",
        "            key = m.group(1).encode('utf-8')\n",
        "            value = m.group(2)\n",
        "            props[key] = [value.encode('utf-8')]\n",
        "        nodes.append(props)\n",
        "    if not nodes:\n",
        "        raise ValueError(\"No nodes found in SGF file\")\n",
        "\n",
        "    # 最初のノードをroot、残りをnodesとして返す\n",
        "    return {\"root\": nodes[0], \"nodes\": nodes[1:]}\n",
        "\n",
        "def build_input_from_history(history, current_player, board_size, history_length):\n",
        "    \"\"\"\n",
        "    棋譜の履歴からネットワークの入力テンソルを構築する。\n",
        "    ・各履歴について、黒石か白石かをチャネルとして展開（各盤面は2チャネル）\n",
        "    ・さらに現在のプレイヤー情報を1チャネル追加\n",
        "    ・最終的に [チャネル数 x 盤面サイズ x 盤面サイズ] のテンソルとなる\n",
        "    \"\"\"\n",
        "    channels = []\n",
        "\n",
        "    for i in range(history_length):\n",
        "        if i < len(history):\n",
        "            board = history[-(i+1)]  # 最新の盤面から遡って取得\n",
        "        else:\n",
        "            board = np.zeros((board_size, board_size), dtype=np.float32)  # 履歴が足りない場合は空盤面\n",
        "        # 黒石と白石をそれぞれ別チャネルに変換（1: 白か黒かのマスク）\n",
        "        channels.append((board == 1).astype(np.float32))\n",
        "        channels.append((board == 2).astype(np.float32))\n",
        "\n",
        "    # 現在のプレイヤー情報（黒:1なら全マス1、白:2なら全マス0）\n",
        "    current_plane = np.ones((board_size, board_size), dtype=np.float32) if current_player == 1 else np.zeros((board_size, board_size), dtype=np.float32)\n",
        "    channels.append(current_plane)\n",
        "\n",
        "    # 全チャネルをスタックして1つの配列にまとめる\n",
        "    return np.stack(channels, axis=0)\n",
        "\n",
        "def apply_dihedral_transform(input_array, transform_id):\n",
        "    \"\"\"\n",
        "    盤面（または複数チャネルの配列）に対して、dihedral group（8通りの回転・反転）変換を適用する。\n",
        "    transform_idが0〜3の場合は、回転のみ（90度単位）、\n",
        "    4〜7の場合は左右反転後に回転\n",
        "    \"\"\"\n",
        "    if transform_id < 4:\n",
        "        return np.rot90(input_array, k=transform_id, axes=(1,2))\n",
        "    else:\n",
        "        flipped = np.flip(input_array, axis=2)  # 横反転\n",
        "        return np.rot90(flipped, k=transform_id-4, axes=(1,2))\n",
        "\n",
        "def transform_policy(target_policy, transform_id, board_size):\n",
        "    \"\"\"\n",
        "    ターゲットポリシー（確率分布またはone-hot表現）に対して、dihedral変換を適用する関数。\n",
        "    ・着手がパス（盤面の外部インデックス）なら変更しない。\n",
        "    ・それ以外の場合、着手位置に対応するインデックスを回転・反転により変換する。\n",
        "    \"\"\"\n",
        "    idx = np.argmax(target_policy)\n",
        "    if idx == board_size * board_size:\n",
        "        return target_policy  # パスの場合はそのまま\n",
        "    row = idx // board_size\n",
        "    col = idx % board_size\n",
        "\n",
        "    # 単一の石が置かれた盤面を作成して変換\n",
        "    board = np.zeros((board_size, board_size), dtype=np.float32)\n",
        "    board[row, col] = 1.0\n",
        "    transformed_board = apply_dihedral_transform(board[np.newaxis, ...], transform_id)[0]\n",
        "    new_idx = np.argmax(transformed_board)\n",
        "    new_policy = np.zeros_like(target_policy)\n",
        "    new_policy[new_idx] = 1.0\n",
        "\n",
        "    return new_policy\n",
        "\n",
        "# ==============================\n",
        "# 盤面クラス（囲碁盤の状態管理）\n",
        "# ==============================\n",
        "class Board:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        # 盤面状態をゼロ初期化（0: 空、1: 黒、2: 白）\n",
        "        self.board = np.zeros((size, size), dtype=np.int8)\n",
        "\n",
        "    def neighbors(self, row, col):\n",
        "        \"\"\"\n",
        "        指定位置の上下左右の隣接マスを列挙する\n",
        "        \"\"\"\n",
        "        for dr, dc in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
        "            r, c = row+dr, col+dc\n",
        "            if 0 <= r < self.size and 0 <= c < self.size:\n",
        "                yield (r, c)\n",
        "\n",
        "    def get_group(self, row, col):\n",
        "        \"\"\"\n",
        "        与えられた位置から、同色の連（グループ）とそのグループが持つ呼吸点（隣接する空点）を取得する。\n",
        "        深さ優先探索で実装。\n",
        "        \"\"\"\n",
        "        color = self.board[row, col]\n",
        "        group = []\n",
        "        liberties = set()\n",
        "        stack = [(row, col)]\n",
        "        visited = set()\n",
        "        while stack:\n",
        "            r, c = stack.pop()\n",
        "            if (r, c) in visited:\n",
        "                continue\n",
        "            visited.add((r, c))\n",
        "            group.append((r, c))\n",
        "            for nr, nc in self.neighbors(r, c):\n",
        "                if self.board[nr, nc] == 0:\n",
        "                    liberties.add((nr, nc))\n",
        "                elif self.board[nr, nc] == color and (nr, nc) not in visited:\n",
        "                    stack.append((nr, nc))\n",
        "        return group, liberties\n",
        "\n",
        "    def play(self, move, color):\n",
        "        \"\"\"\n",
        "        指定した場所に石を置き、相手石の捕獲や自分の石の自殺手の処理を行う。\n",
        "        ・置く場所にすでに石があればエラーを投げる\n",
        "        ・着手後、隣接している相手連の呼吸点を確認し、呼吸点がなければその連を盤面から除去\n",
        "        ・その後、自分の連に呼吸点がなけければ自殺手として自身の連を除去\n",
        "        \"\"\"\n",
        "        row, col = move\n",
        "        if self.board[row, col] != 0:\n",
        "            raise ValueError(\"Illegal move: position already occupied\")\n",
        "\n",
        "        # color 'b'なら1, 'w'なら2として内部的に管理\n",
        "        stone = 1 if color=='b' else 2\n",
        "        self.board[row, col] = stone\n",
        "\n",
        "        # 相手の石の色を決定\n",
        "        opponent = 2 if stone==1 else 1\n",
        "\n",
        "        # 隣接マスについて相手連をチェックし、呼吸点がなければ除去\n",
        "        for nr, nc in self.neighbors(row, col):\n",
        "            if self.board[nr, nc] == opponent:\n",
        "                group, liberties = self.get_group(nr, nc)\n",
        "                if len(liberties) == 0:\n",
        "                    for r, c in group:\n",
        "                        self.board[r, c] = 0\n",
        "\n",
        "        # 自身が置いた石についても呼吸点がなければ取り除く（自殺手の検出）\n",
        "        group, liberties = self.get_group(row, col)\n",
        "        if len(liberties) == 0:\n",
        "            for r, c in group:\n",
        "                self.board[r, c] = 0\n",
        "\n",
        "# ==============================\n",
        "# Datasetクラス\n",
        "# ==============================\n",
        "class AlphaZeroSGFDatasetPreloaded(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch用のDatasetクラス。前処理済みのSGFサンプルをメモリ上にロードしたもの。\n",
        "    各サンプルは、盤面データ（flattenしたテンソル）、ターゲットポリシー、ターゲット値、マージンを含む。\n",
        "    \"\"\"\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, pol, val, mar = self.samples[idx]\n",
        "        # 盤面データをテンソル化し、ネットワーク入力の形状に変換\n",
        "        board_tensor = torch.tensor(inp, dtype=torch.float32).view(NUM_CHANNELS, BOARD_SIZE, BOARD_SIZE)\n",
        "        target_policy_tensor = torch.tensor(pol, dtype=torch.float32)\n",
        "        target_value_tensor = torch.tensor(val, dtype=torch.float32)\n",
        "        target_margin_tensor = torch.tensor(mar, dtype=torch.float32)\n",
        "        return board_tensor, target_policy_tensor, target_value_tensor, target_margin_tensor\n",
        "\n",
        "# ==============================\n",
        "# SGFからサンプル生成関数\n",
        "# ==============================\n",
        "def process_sgf_to_samples_from_text(sgf_src, board_size, history_length, augment_all):\n",
        "    \"\"\"\n",
        "    1つのSGF文字列から、複数の学習サンプルを生成する関数。\n",
        "    ・SGFファイルをパースして、各ノードごとに盤面履歴や着手情報を取り出す。\n",
        "    ・データ拡張（dihedral変換）を適用できる（augment_allフラグ）\n",
        "    ・最終的なサンプルは、flattenした盤面、one-hotなターゲットポリシー、ターゲット値、マージンの組み合わせ\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    try:\n",
        "        sgf_data = parse_sgf(sgf_src)\n",
        "    except Exception as e:\n",
        "        sgf_logger.error(f\"Error processing SGF text: {e}\")\n",
        "        return samples\n",
        "    root = sgf_data[\"root\"]\n",
        "\n",
        "    try:\n",
        "        # 盤面サイズはrootプロパティ「SZ」から取得。取得失敗時は引数のboard_sizeを使用\n",
        "        sz = int(root.get(b'SZ')[0].decode('utf-8'))\n",
        "    except Exception:\n",
        "        sz = board_size\n",
        "\n",
        "    # 勝敗情報：「RE」プロパティを取得。なければ\"不明\"\n",
        "    result_prop = root.get(b'RE') if b'RE' in root else None\n",
        "    result_str = result_prop[0].decode('utf-8') if result_prop and len(result_prop)>0 else \"不明\"\n",
        "\n",
        "    # 勝敗の値として、黒勝ちなら1.0、白勝ちなら-1.0、引き分け等は0.0に設定\n",
        "    target_value = 1.0 if result_str.startswith(\"B+\") else -1.0 if result_str.startswith(\"W+\") else 0.0\n",
        "    try:\n",
        "        # margin（点差等）の取得。失敗したら0.0\n",
        "        target_margin = float(result_str[2:]) if result_str[2:] else 0.0\n",
        "    except Exception:\n",
        "        target_margin = 0.0\n",
        "\n",
        "    board_obj = Board(sz)\n",
        "\n",
        "    # 初期盤面（空盤）を履歴に追加\n",
        "    history_boards = [board_obj.board.copy().astype(np.float32)]\n",
        "    current_player = 1  # 初手は黒（1）から開始\n",
        "\n",
        "    # SGFデータ内の各ノードを処理\n",
        "    for node in sgf_data[\"nodes\"]:\n",
        "\n",
        "        # 現在のプレイヤーに応じて、着手プロパティ（B: 黒, W: 白）を決定\n",
        "        move_prop = b'B' if current_player==1 else b'W'\n",
        "        move_vals = node.get(move_prop)\n",
        "\n",
        "        # 現在の盤面履歴からネットワークの入力テンソルを構築\n",
        "        input_tensor = build_input_from_history(history_boards, current_player, sz, history_length)\n",
        "        if move_vals is None or len(move_vals)==0 or move_vals[0]==b\"\":\n",
        "            # 着手がない場合はパス（最後のインデックスを1に）\n",
        "            target_policy = np.zeros(sz*sz+1, dtype=np.float32)\n",
        "            target_policy[sz*sz] = 1.0\n",
        "        else:\n",
        "            try:\n",
        "                # 着手情報のデコード。2文字（a～s等と仮定）で行インデックス、列インデックスを計算\n",
        "                move = move_vals[0]\n",
        "                col = ord(move.decode('utf-8')[0])-ord('a')\n",
        "                row = ord(move.decode('utf-8')[1])-ord('a')\n",
        "                target_policy = np.zeros(sz*sz+1, dtype=np.float32)\n",
        "                target_policy[row*sz+col] = 1.0  # 対応するマスを1に\n",
        "            except Exception as e:\n",
        "                sgf_logger.warning(f\"Error parsing move in SGF text: {e}\")\n",
        "                target_policy = np.zeros(sz*sz+1, dtype=np.float32)\n",
        "                target_policy[sz*sz] = 1.0\n",
        "\n",
        "        # データ拡張：augment_allがTrueなら8通り、Falseならランダム1通り\n",
        "        transforms = range(8) if augment_all else [np.random.randint(0,8)]\n",
        "        for t in transforms:\n",
        "            inp = apply_dihedral_transform(input_tensor, t)\n",
        "            pol = transform_policy(target_policy, t, sz)\n",
        "            samples.append((\n",
        "                inp.flatten(),  # flattenして1次元配列に変換\n",
        "                pol,\n",
        "                np.array([target_value], dtype=np.float32),\n",
        "                np.array([target_margin], dtype=np.float32)\n",
        "            ))\n",
        "\n",
        "        # 着手が有る場合、盤面状態を更新し、履歴に追加\n",
        "        if move_vals is not None and len(move_vals)>0 and move_vals[0]!=b\"\":\n",
        "            try:\n",
        "                move = move_vals[0]\n",
        "                col = ord(move.decode('utf-8')[0])-ord('a')\n",
        "                row = ord(move.decode('utf-8')[1])-ord('a')\n",
        "                board_obj.play((row, col), 'b' if current_player==1 else 'w')\n",
        "                history_boards.append(board_obj.board.copy().astype(np.float32))\n",
        "            except Exception as e:\n",
        "                sgf_logger.warning(f\"Error updating board from SGF text: {e}\")\n",
        "\n",
        "        # プレイヤー交代\n",
        "        current_player = 2 if current_player==1 else 1\n",
        "\n",
        "    return samples\n",
        "\n",
        "# ==============================\n",
        "# 最良モデル保存用関数\n",
        "# ==============================\n",
        "def save_best_model(model, policy_accuracy, device, current_best_accuracy):\n",
        "    \"\"\"\n",
        "    現在のPolicy Accuracyがこれまでの最高値を更新した場合、以下の処理を実施する：\n",
        "      - 状態辞書形式でモデルを保存する\n",
        "      - 推論専用モデル（TorchScript化）の保存\n",
        "      - MODEL_OUTPUT_DIR内の精度が低いモデルファイルの削除\n",
        "      - 新たな最高精度を返す\n",
        "    \"\"\"\n",
        "    new_model_file = os.path.join(MODEL_OUTPUT_DIR, f\"model_{policy_accuracy:.5f}.pt\")\n",
        "    # モデルの状態辞書を保存\n",
        "    torch.save(model.state_dict(), new_model_file)\n",
        "    train_logger.info(f\"● New best model saved (state_dict): {new_model_file}\")\n",
        "\n",
        "    # 推論専用モデルのTorchScript化と保存\n",
        "    save_inference_model(model, device, \"inference2_model.pt\")\n",
        "\n",
        "    # モデル出力ディレクトリ内の他の低精度モデルファイルを削除\n",
        "    for f in os.listdir(MODEL_OUTPUT_DIR):\n",
        "        if f.startswith(\"model_\") and f.endswith(\".pt\"):\n",
        "            try:\n",
        "                acc = float(f[len(\"model_\"):-len(\".pt\")])\n",
        "                if acc < current_best_accuracy and os.path.join(MODEL_OUTPUT_DIR, f) != new_model_file:\n",
        "                    os.remove(os.path.join(MODEL_OUTPUT_DIR, f))\n",
        "                    train_logger.info(f\"Deleted old model: {f}\")\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    return max(policy_accuracy, current_best_accuracy)\n",
        "\n",
        "# ==============================\n",
        "# データセットの保存／読み込み関数\n",
        "# ==============================\n",
        "def save_dataset(samples, output_file):\n",
        "    # pickle形式でサンプル群を保存\n",
        "    with open(output_file, \"wb\") as f:\n",
        "        pickle.dump(samples, f)\n",
        "    sgf_logger.info(f\"Saved dataset to {output_file}\")\n",
        "\n",
        "def load_dataset(output_file):\n",
        "    # pickle形式からサンプル群をロード\n",
        "    with open(output_file, \"rb\") as f:\n",
        "        samples = pickle.load(f)\n",
        "    sgf_logger.info(f\"Loaded dataset from {output_file}\")\n",
        "    return samples\n",
        "\n",
        "# ==============================\n",
        "# 推論専用モデル（TorchScript）保存関数\n",
        "# ==============================\n",
        "def save_inference_model(model, device, model_name):\n",
        "    \"\"\"\n",
        "    モデルをCPUに移動して、torch.jit.traceを用いてTorchScript形式の推論専用モデルを生成・保存する。\n",
        "    ・トレース用のダミー入力を用いて変換し、\n",
        "    ・保存後、モデルを元のデバイスに戻す。\n",
        "    \"\"\"\n",
        "    model_cpu = model.cpu()  # CPUへ移動\n",
        "    dummy_input = torch.randn(1, NUM_CHANNELS, BOARD_SIZE, BOARD_SIZE, device=torch.device(\"cpu\"))\n",
        "    traced_module = torch.jit.trace(model_cpu, dummy_input)\n",
        "    inference_model_file = os.path.join(MODEL_OUTPUT_DIR, model_name)\n",
        "    torch.jit.save(traced_module, inference_model_file)\n",
        "    train_logger.info(f\"Inference model saved as {inference_model_file}\")\n",
        "    model.to(device)  # 元のデバイスに戻す\n",
        "\n",
        "# ==============================\n",
        "# Test用データセット生成（zip利用）\n",
        "# ==============================\n",
        "def prepare_test_dataset(sgf_dir, board_size, history_length, augment_all, output_file):\n",
        "    \"\"\"\n",
        "    テスト用のデータセットを生成する関数。\n",
        "    ・既にpickleファイルが存在する場合はそれをロード。\n",
        "    ・無ければ、SGFファイルからzipアーカイブを作成し、そこからサンプルを生成。\n",
        "    ・生成したサンプルはpickle形式で保存する。\n",
        "    \"\"\"\n",
        "    if os.path.exists(output_file):\n",
        "        sgf_logger.info(f\"Test dataset pickle {output_file} already exists. Loading it directly...\")\n",
        "        return load_dataset(output_file)\n",
        "\n",
        "    if not os.path.exists(TEST_SGFS_ZIP):\n",
        "        sgf_logger.info(f\"Creating zip archive {TEST_SGFS_ZIP} from SGF files in {sgf_dir} ...\")\n",
        "        sgf_files = [os.path.join(sgf_dir, f) for f in os.listdir(sgf_dir)\n",
        "                     if f.endswith('.sgf') and \"analyzed\" not in f.lower()]\n",
        "        with zipfile.ZipFile(TEST_SGFS_ZIP, 'w') as zf:\n",
        "            for filepath in sgf_files:\n",
        "                zf.write(filepath, arcname=os.path.basename(filepath))\n",
        "        sgf_logger.info(f\"Zip archive created: {TEST_SGFS_ZIP}\")\n",
        "    else:\n",
        "        sgf_logger.info(f\"Zip archive {TEST_SGFS_ZIP} already exists. Loading from it...\")\n",
        "\n",
        "    all_samples = []\n",
        "\n",
        "    with zipfile.ZipFile(TEST_SGFS_ZIP, 'r') as zf:\n",
        "        sgf_names = [name for name in zf.namelist() if name.endswith('.sgf') and \"analyzed\" not in name.lower()]\n",
        "        sgf_names.sort()\n",
        "        sgf_logger.info(f\"TEST: Total SGF files in zip to process: {len(sgf_names)}\")\n",
        "        for name in tqdm(sgf_names, desc=\"Processing TEST SGF files\"):\n",
        "            try:\n",
        "                sgf_src = zf.read(name).decode('utf-8')\n",
        "                file_samples = process_sgf_to_samples_from_text(sgf_src, board_size, history_length, augment_all=False)\n",
        "                all_samples.extend(file_samples)\n",
        "            except Exception as e:\n",
        "                sgf_logger.error(f\"Error processing {name} from zip: {e}\")\n",
        "\n",
        "    save_dataset(all_samples, output_file)\n",
        "    sgf_logger.info(f\"TEST: Saved test dataset (total samples: {len(all_samples)}) to {output_file}\")\n",
        "\n",
        "    return all_samples\n",
        "\n",
        "# ==============================\n",
        "# グローバル変数：未処理のSGFファイルリスト\n",
        "# ==============================\n",
        "remaining_sgf_files = []\n",
        "\n",
        "def prepare_train_dataset_cycle(sgf_dir, board_size, history_length, augment_all, max_files):\n",
        "    \"\"\"\n",
        "    指定フォルダ内のSGFファイルから、1サイクル分の学習サンプルを生成する関数。\n",
        "    ・全SGFファイルをランダム順に並び替え、max_files件分だけ処理する。\n",
        "    ・ファイルごとにSGFテキストを読み込み、サンプルを生成する。\n",
        "    \"\"\"\n",
        "    global remaining_sgf_files\n",
        "    if not remaining_sgf_files:\n",
        "        all_files = [os.path.join(sgf_dir, f) for f in os.listdir(sgf_dir)\n",
        "                     if f.endswith('.sgf') and \"analyzed\" not in f.lower()]\n",
        "        random.shuffle(all_files)\n",
        "        remaining_sgf_files = all_files\n",
        "        sgf_logger.info(\"Regenerated the random order of all SGF files.\")\n",
        "\n",
        "    if len(remaining_sgf_files) < max_files:\n",
        "        selected_files = remaining_sgf_files\n",
        "        remaining_sgf_files = []  # 全部使い切る\n",
        "        sgf_logger.info(f\"Remaining SGF files less than max_files ({max_files}). Processing {len(selected_files)} files.\")\n",
        "    else:\n",
        "        selected_files = remaining_sgf_files[:max_files]\n",
        "        remaining_sgf_files = remaining_sgf_files[max_files:]\n",
        "        sgf_logger.info(f\"Selected {len(selected_files)} SGF files.\")\n",
        "\n",
        "    all_samples = []\n",
        "\n",
        "    for sgf_file in selected_files:\n",
        "        try:\n",
        "            with open(sgf_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                sgf_src = f.read()\n",
        "            file_samples = process_sgf_to_samples_from_text(sgf_src, board_size, history_length, augment_all)\n",
        "            all_samples.extend(file_samples)\n",
        "        except Exception as e:\n",
        "            sgf_logger.error(f\"Error processing file {sgf_file}: {e}\")\n",
        "\n",
        "    random.shuffle(all_samples)\n",
        "    sgf_logger.info(f\"Training dataset cycle created. Total samples: {len(all_samples)}\")\n",
        "\n",
        "    return all_samples\n",
        "\n",
        "def load_training_dataset(sgf_dir, board_size, history_length, augment_all, max_files):\n",
        "    \"\"\"\n",
        "    トレーニング用のデータセットを一度だけ生成し、AlphaZeroSGFDatasetPreloadedのインスタンスとして返す。\n",
        "    \"\"\"\n",
        "    samples = prepare_train_dataset_cycle(sgf_dir, board_size, history_length, augment_all, max_files)\n",
        "    dataset = AlphaZeroSGFDatasetPreloaded(samples)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def validate_model(model, test_loader, device):\n",
        "    \"\"\"\n",
        "    テスト用データセットを用いてモデルのpolicy accuracyを計算する関数。\n",
        "    各バッチごとに、モデルの予測とターゲットのインデックスを比較し、正解数をカウントする。\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for boards, target_policies, _, _ in tqdm(test_loader, desc=\"Validation\", bar_format=bar_fmt):\n",
        "            boards = boards.to(device)\n",
        "            target_policies = target_policies.to(device)\n",
        "            pred_policy, _ = model(boards)\n",
        "            # 各サンプルについて、予測とターゲットの最大値のインデックスを比較\n",
        "            correct = (pred_policy.argmax(dim=1) == target_policies.argmax(dim=1)).sum().item()\n",
        "            total_correct += correct\n",
        "            total_samples_count += boards.size(0)\n",
        "\n",
        "    policy_accuracy = total_correct / total_samples_count\n",
        "    train_logger.info(f\"===== Validation Policy Accuracy ==== 【{policy_accuracy:.5f}】\")\n",
        "\n",
        "    return policy_accuracy\n",
        "\n",
        "# ==============================\n",
        "# 訓練ループ用関数（1エポック分）\n",
        "# ==============================\n",
        "def train_one_iteration(model, train_loader, optimizer, device):\n",
        "    \"\"\"\n",
        "    1エポック分の訓練ループを実行する関数\n",
        "    ・各バッチごとに損失の計算、逆伝播、パラメータ更新を行う\n",
        "    ・policy loss, value loss, margin lossを各々計算して最終損失に加算する\n",
        "    ・バッチごとの正解率も計算してログ出力する\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_policy_loss = 0.0\n",
        "    total_value_loss = 0.0\n",
        "    total_margin_loss = 0.0\n",
        "    num_batches = 0\n",
        "    overall_correct = 0\n",
        "    overall_samples = 0\n",
        "\n",
        "    # 損失の重み（ハイパーパラメータ）\n",
        "    value_loss_coefficient = 0.1\n",
        "    margin_loss_coefficient = 0.0001\n",
        "\n",
        "    print_interval = 100  # ログ出力するバッチ数の間隔\n",
        "    accumulated_accuracy = 0.0\n",
        "    group_batches = 0\n",
        "\n",
        "    for boards, target_policies, target_values, target_margins in tqdm(train_loader, desc=\"Training\", bar_format=bar_fmt):\n",
        "        boards = boards.to(device)\n",
        "        target_policies = target_policies.to(device)\n",
        "        target_values = target_values.to(device)\n",
        "        target_margins = target_margins.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # 勾配の初期化\n",
        "        pred_policy, (pred_value, pred_margin) = model(boards)\n",
        "\n",
        "        # ポリシー損失は、ターゲットの対数確率との内積による負の和をバッチ数で割る\n",
        "        policy_loss = -torch.sum(target_policies * pred_policy) / boards.size(0)\n",
        "\n",
        "        # 値とマージンに対する平均二乗誤差\n",
        "        value_loss = F.mse_loss(pred_value.view(-1), target_values.view(-1))\n",
        "        margin_loss = F.mse_loss(pred_margin.view(-1), target_margins.view(-1))\n",
        "        loss = policy_loss + value_loss_coefficient * value_loss + margin_loss_coefficient * margin_loss\n",
        "        loss.backward()  # 逆伝播\n",
        "        optimizer.step()  # パラメータ更新\n",
        "\n",
        "        if USE_TPU:\n",
        "            xm.mark_step()  # TPUの場合、明示的にステップをマークする必要がある\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_policy_loss += policy_loss.item()\n",
        "        total_value_loss += value_loss.item()\n",
        "        total_margin_loss += margin_loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # バッチごとに正解率（予測クラスとターゲットクラスの一致）を計算\n",
        "        batch_pred = pred_policy.argmax(dim=1)\n",
        "        batch_target = target_policies.argmax(dim=1)\n",
        "        batch_accuracy = (batch_pred == batch_target).float().mean().item()\n",
        "        overall_correct += (batch_pred == batch_target).sum().item()\n",
        "        overall_samples += boards.size(0)\n",
        "        accumulated_accuracy += batch_accuracy\n",
        "\n",
        "        group_batches += 1\n",
        "\n",
        "        if num_batches % print_interval == 0:\n",
        "            avg_accuracy = accumulated_accuracy / group_batches\n",
        "            start_batch = num_batches - group_batches + 1\n",
        "            end_batch = num_batches\n",
        "            print(f\"Batch {start_batch:4d}～{end_batch:4d} policy accuracy average: {avg_accuracy:6.4f}\")\n",
        "            accumulated_accuracy = 0.0\n",
        "            group_batches = 0\n",
        "\n",
        "        del boards, target_policies, target_values, target_margins\n",
        "\n",
        "    if group_batches > 0:\n",
        "        avg_accuracy = accumulated_accuracy / group_batches\n",
        "        print(f\"Other ({group_batches} batch) policy accuracy average: {avg_accuracy:6.4f}\")\n",
        "\n",
        "    if overall_samples > 0:\n",
        "        overall_accuracy = overall_correct / overall_samples\n",
        "        print(f\"Overall policy accuracy of the latest model state in this training loop: {overall_accuracy:6.4f}\")\n",
        "    else:\n",
        "        overall_accuracy = 0.0\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_policy_loss = total_policy_loss / num_batches\n",
        "    avg_value_loss = value_loss_coefficient * total_value_loss / num_batches\n",
        "    avg_margin_loss = margin_loss_coefficient * total_margin_loss / num_batches\n",
        "\n",
        "    train_logger.info(f\"Training iteration  total average loss: {avg_loss:.5f}\")\n",
        "    train_logger.info(f\"Training iteration average policy loss: {avg_policy_loss:.5f}\")\n",
        "    train_logger.info(f\"Training iteration  average value loss: {avg_value_loss:.5f}\")\n",
        "    train_logger.info(f\"Training iteration average margin loss: {avg_margin_loss:.5f}\")\n",
        "    train_logger.info(f\"Training iteration  overall p accuracy: {overall_accuracy:.5f}\")\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "# ==============================\n",
        "# チェックポイント保存＆復元\n",
        "# ==============================\n",
        "def save_checkpoint(model, optimizer, epoch, best_val_loss, epochs_no_improve, best_policy_accuracy, checkpoint_file, device):\n",
        "    \"\"\"\n",
        "    モデルの状態（パラメータやオプティマイザ情報）をチェックポイントとして保存する。\n",
        "    \"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'epochs_no_improve': epochs_no_improve,\n",
        "        'best_policy_accuracy': best_policy_accuracy\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, checkpoint_file)\n",
        "\n",
        "    train_logger.info(f\"Checkpoint saved at epoch {epoch} to {checkpoint_file}\")\n",
        "\n",
        "def recursive_to(data, device):\n",
        "    \"\"\"\n",
        "    ネストされたデータ構造内の全てのtorch.Tensorを指定デバイスに移動するヘルパー関数\n",
        "    \"\"\"\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        return data.to(device)\n",
        "    elif isinstance(data, dict):\n",
        "        return {k: recursive_to(v, device) for k, v in data.items()}\n",
        "    elif isinstance(data, list):\n",
        "        return [recursive_to(item, device) for item in data]\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "def load_checkpoint(model, optimizer, checkpoint_file, device):\n",
        "    \"\"\"\n",
        "    チェックポイントファイルからモデルとオプティマイザの状態を復元する。\n",
        "    \"\"\"\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        checkpoint = torch.load(checkpoint_file, map_location=torch.device(\"cpu\"))\n",
        "\n",
        "        # チェックポイント内の各パラメータを指定デバイスに移動\n",
        "        new_state_dict = {k: v.to(device) for k, v in checkpoint['model_state_dict'].items()}\n",
        "        model.load_state_dict(new_state_dict)\n",
        "\n",
        "        optimizer_state = recursive_to(checkpoint['optimizer_state_dict'], device)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "        epoch = checkpoint['epoch']\n",
        "        best_policy_accuracy = checkpoint.get('best_policy_accuracy', 0.0)\n",
        "\n",
        "        train_logger.info(f\"Checkpoint loaded from {checkpoint_file} at epoch {epoch}\")\n",
        "\n",
        "        return epoch, best_policy_accuracy\n",
        "    else:\n",
        "        train_logger.info(\"No checkpoint found. Starting from scratch.\")\n",
        "        return 0, 0.0\n",
        "\n",
        "# ==============================\n",
        "# TPU分散環境で動作するメイン処理\n",
        "# ==============================\n",
        "def _mp_fn(rank):\n",
        "    \"\"\"\n",
        "    TPU分散環境（もしくはシングルプロセス）で実行されるメインの関数\n",
        "    ・デバイスの設定、データセットの生成、モデルの構築、チェックポイントの復元、\n",
        "    ・学習ループおよび評価処理を行う。\n",
        "    \"\"\"\n",
        "    if USE_TPU:\n",
        "        if not dist.is_initialized():\n",
        "            dist.init_process_group(\"xla\", init_method='xla://')\n",
        "        device = xm.xla_device()\n",
        "        train_logger.info(\"Running on TPU device: {}\".format(device))\n",
        "    else:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        train_logger.info(\"Running on device: {}\".format(device))\n",
        "\n",
        "    # テスト用データセットのpickleファイルのパス設定\n",
        "    test_dataset_pickle = os.path.join(VAL_SGF_DIR, \"test_dataset.pkl\")\n",
        "\n",
        "    # テスト用サンプルを生成またはロード\n",
        "    test_samples = prepare_test_dataset(VAL_SGF_DIR, BOARD_SIZE, HISTORY_LENGTH, True, test_dataset_pickle)\n",
        "    test_dataset = AlphaZeroSGFDatasetPreloaded(test_samples)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # ネットワークのインスタンス生成し、指定デバイスへ移動\n",
        "    model = EnhancedResNetPolicyValueNetwork(\n",
        "        board_size=BOARD_SIZE,\n",
        "        num_channels=model_channels,\n",
        "        num_residual_blocks=num_residual_blocks,\n",
        "        in_channels=NUM_CHANNELS\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # 学習率スケジューラの設定（評価指標が停滞した場合に減衰）\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience, factor=factor)\n",
        "\n",
        "    # チェックポイントから復元\n",
        "    start_epoch, best_policy_accuracy = load_checkpoint(model, optimizer, CHECKPOINT_FILE, device)\n",
        "\n",
        "    train_logger.info(\"Initial best_policy_accuracy: {:.5f}\".format(best_policy_accuracy))\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    train_logger.info(\"Current learning rate : {:.8f}\".format(current_lr))\n",
        "\n",
        "    # トレーニング用データセットの生成\n",
        "    training_dataset = load_training_dataset(TRAIN_SGF_DIR, BOARD_SIZE, HISTORY_LENGTH, augment_all=True, max_files=number_max_files)\n",
        "\n",
        "    epoch = start_epoch\n",
        "\n",
        "    # 無限ループで学習・評価・チェックポイント保存を繰り返す\n",
        "    while True:\n",
        "        train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
        "        train_one_iteration(model, train_loader, optimizer, device)\n",
        "        epoch += 1\n",
        "        policy_accuracy = validate_model(model, test_loader, device)\n",
        "        if policy_accuracy > best_policy_accuracy:\n",
        "            best_policy_accuracy = save_best_model(model, policy_accuracy, device, best_policy_accuracy)\n",
        "        else:\n",
        "            # 改良がなかった場合でも一時的な推論用モデルを保存\n",
        "            save_inference_model(model, device, \"inference2_model_tmp.pt\")\n",
        "\n",
        "        lr_before = optimizer.param_groups[0]['lr']\n",
        "        train_logger.info(\"Epoch {} - Before scheduler.step(): lr = {:.8f}\".format(epoch, lr_before))\n",
        "        scheduler.step(policy_accuracy)\n",
        "        lr_after = optimizer.param_groups[0]['lr']\n",
        "        train_logger.info(\"Epoch {} - After scheduler.step(): lr = {:.8f}\".format(epoch, lr_after))\n",
        "\n",
        "        # ダミーの評価損失、エポック不改善回数（本実装では利用していない）を用いてチェックポイント保存\n",
        "        dummy_best_val_loss = 0.0\n",
        "        dummy_epochs_no_improve = 0\n",
        "        save_checkpoint(model, optimizer, epoch, dummy_best_val_loss, dummy_epochs_no_improve, best_policy_accuracy, CHECKPOINT_FILE, device)\n",
        "        train_logger.info(\"Iteration completed. Restarting next iteration...\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # コマンドライン引数の設定（設定ファイルとチェックポイントファイルのパスを指定可能）\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--config\", type=str, default=os.path.join(BASE_DIR, \"config_py.ini\"),\n",
        "                        help=\"Path to configuration file\")\n",
        "    parser.add_argument(\"--checkpoint\", type=str, default=CHECKPOINT_FILE,\n",
        "                        help=\"Path to checkpoint file\")\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    if not os.path.exists(args.config):\n",
        "        sgf_logger.warning(\"Config file not found. Using default hyperparameters.\")\n",
        "    train_logger.info(\"=== Starting Training and Validation Loop ===\")\n",
        "\n",
        "    if USE_TPU:\n",
        "        # TPU利用時は複数プロセス起動のため、xmp.spawnを利用（ここではnprocs=1）\n",
        "        import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "        nprocs = 1\n",
        "        xmp.spawn(_mp_fn, args=(), nprocs=nprocs)\n",
        "    else:\n",
        "        _mp_fn(0)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNfjJqM8Eo0fL1ONCLdPoFO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}